{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Parsa33033/ADM_HW3/blob/main/homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR-dxW-68OXP",
    "outputId": "f197a5c5-f6a8-4303-a0dd-7fd5215e53cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "rkVdjQff8ggD"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "import heapq as hq\n",
    "import matplotlib as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "5F7tX0Mi81Ir"
   },
   "outputs": [],
   "source": [
    "num_of_pages = 384 \n",
    "directory = \"\"\n",
    "links_file = \"anime.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the code down below to collect the page, name and URLs of the animes in a .csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXKNsnC682xk",
    "outputId": "a9369342-4bc3-41aa-e9b8-d40eb636b0d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 384/384 [02:53<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "anime = []\n",
    "\n",
    "                               \n",
    "if not os.path.isfile(links_file):\n",
    "    with open(directory + links_file, \"w\", encoding='utf-8', newline='') as fobj:\n",
    "        fobj.write(\"page,name,url\\n\")\n",
    "        for page in tqdm(range(0, num_of_pages)):\n",
    "            url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "            response = requests.get(url)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for tag in soup.find_all('tr'):\n",
    "                links = tag.find_all('a')\n",
    "                for link in links:        \n",
    "                    if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                        x = (page + 1, link.contents[0], link.get('href'))\n",
    "                        anime.append(x)\n",
    "                        writer = csv.writer(fobj)\n",
    "                        writer.writerow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawling through the URLs proved to be a problem, since after crawling through a certain number of pages with the same IP address we got security constrained: the block comes through a status code that starts with 400 (which indicates a client-side problem). To solve this issue, we checked the status code of each request while crawling through the pages. If that status code started with 400, we would stop the function for a certain amount of seconds and try again, until the status code turned to 200.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_dir = directory + \"pages\"\n",
    "page_dir = pages_dir + \"/\" + \"page\"\n",
    "article = \"article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "ZYKNkYqf84p_",
    "outputId": "82b6bd03-2088-4b25-da8c-152586d6766b"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(pages_dir):\n",
    "    os.makedirs(pages_dir)\n",
    "    \n",
    "def urlEncodeNonAscii(b):\n",
    "    return re.sub('[\\x80-\\xFF]', lambda c: '%%%02x' % ord(c.group(0)), b)\n",
    "\n",
    "ds = pd.read_csv(directory + links_file)\n",
    "i = 1\n",
    "for p, u in tqdm(zip(ds.page, ds.url)):\n",
    "    d = page_dir + str(p)\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "    d = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "    \n",
    "    if not os.path.exists(d):\n",
    "#         request = requests.get(u, headers={'Cache-Control': 'no-cache', 'User-Agent': 'Mozilla/5.0'})\n",
    "        req = Request(u, headers={'User-Agent': 'XYZ/3.0'})\n",
    "        webpage = urlopen(req, timeout=100).read()\n",
    "        with open(d, \"w\", encoding='utf-8') as fobj:\n",
    "            fobj.write(webpage)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9WbbqyPD__n"
   },
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(parent):\n",
    "    return ''.join(parent.find_all(text=True, recursive=False)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_parser(article_path, soup):\n",
    "        anime_title = soup.title.text.strip()\n",
    "        anime_type = \"\"\n",
    "        try:\n",
    "            anime_type = getText(soup.find('span', text=\"Type:\").parent.a)\n",
    "        except:\n",
    "            pass\n",
    "        anime_num_episodes = \"\"\n",
    "        try:\n",
    "            anime_num_episodes = getText(soup.find('span', text=\"Episodes:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_aired = \"\"\n",
    "        try:\n",
    "            anime_aired = getText(soup.find('span', text=\"Aired:\").parent).split(\" to \")\n",
    "        except:\n",
    "            pass\n",
    "        anime_status = \"\"\n",
    "        try:\n",
    "            anime_status = getText(soup.find('span', text=\"Status:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_score = \"\"\n",
    "        try:\n",
    "            anime_score = getText(soup.find('span', text=\"Score:\").parent.find_all('span', {'class': 'score-label'})[0])\n",
    "        except:\n",
    "            pass\n",
    "        anime_users = \"\"\n",
    "        try:\n",
    "            anime_users = getText(soup.find('span', text=\"Members:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_rank = \"\"\n",
    "        try:\n",
    "            anime_rank = getText(soup.find('span', text=\"Ranked:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_popularity = \"\"\n",
    "        try:\n",
    "            anime_popularity = getText(soup.find('span', text=\"Popularity:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_description = \"\"\n",
    "        try:\n",
    "            anime_description = getText(soup.find('h2', text=\"Synopsis\").parent.parent.p)\n",
    "        except:\n",
    "            pass\n",
    "        anime_related = []\n",
    "        try: \n",
    "            anime_related = list(set(map(getText,soup.find('h2', text=\"Related Anime\").parent.parent.tr.find_all(lambda tag:tag.name == \"a\" and tag.href != \"\")))) \n",
    "        except: \n",
    "            pass\n",
    "        anime_characters = []\n",
    "        try:\n",
    "            anime_characters = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[1].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "            anime_characters = [e.split(\", \") for e in anime_characters]\n",
    "        except:\n",
    "            pass\n",
    "        anime_voices = []\n",
    "        try:\n",
    "            anime_voices = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[0].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "            anime_voices = [e.split(\", \") for e in anime_voices]\n",
    "        except:\n",
    "            pass\n",
    "        anime_staff = []\n",
    "        try:\n",
    "            anime_staff = list(map(lambda tr: [re.split(\"\\n+\", tr.text.strip())[0].split(', '), re.split(\"\\n+\", tr.text.strip())[1]], soup.find('h2', text=\"Staff\").find_next('div').find_all('tr')))\n",
    "        except:\n",
    "            pass\n",
    "        l = [anime_title,\n",
    "                anime_type,\n",
    "                anime_num_episodes,\n",
    "                anime_aired,\n",
    "                anime_status,\n",
    "                anime_score,\n",
    "                anime_users,\n",
    "                anime_rank,\n",
    "                anime_popularity,\n",
    "                anime_description,\n",
    "                anime_related,\n",
    "                anime_characters,\n",
    "                anime_voices,\n",
    "                anime_staff]\n",
    "        return l\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a single .tsv file containing the information for each anime. We decided to use a single .tsv file instead of one for each anime because we felt it was the best solution and we felt more comfortable working with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_docs = 19125\n",
    "i = 1\n",
    "with open('anime.csv', 'rb') as urls:\n",
    "    urls_ds = pd.read_csv(urls)\n",
    "    with open('anime.tsv', 'w', encoding=\"utf-8\", newline='') as anime:\n",
    "        for p in range(1, num_of_pages + 1):\n",
    "            flag = True\n",
    "            if (flag): \n",
    "                article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "                while os.path.isfile(article_path):\n",
    "                    article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "                    if os.path.isfile(article_path):\n",
    "                        if i % 100 == 0:\n",
    "                            print(p, \"--->\", i, \"--->\", article_path)\n",
    "                        with open(article_path, 'rb') as html:\n",
    "                            soup = BeautifulSoup(html, \"html.parser\")\n",
    "                            l = article_parser(article_path, soup)\n",
    "                            l.append(urls_ds.url[i - 1])\n",
    "                            l.append(urls_ds.name[i - 1])\n",
    "                            writer = csv.writer(anime, delimiter='\\t')\n",
    "                            writer.writerow(l)\n",
    "                    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first created a better dataframe by adding the correct columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18741"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv(\"anime.tsv\", delimiter='\\t')\n",
    "ds.columns = ['anime_title',\n",
    "                'anime_type',\n",
    "                'anime_num_episodes',\n",
    "                'anime_aired',\n",
    "                'anime_status',\n",
    "                'anime_score',\n",
    "                'anime_users',\n",
    "                'anime_rank',\n",
    "                'anime_popularity',\n",
    "                'anime_description',\n",
    "                'anime_related',\n",
    "                'anime_characters',\n",
    "                'anime_voices',\n",
    "                'anime_staff',\n",
    "                'url',\n",
    "                'title']\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() #this was to install some additional modules we needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the functions to preprocess our data. We removed stopwords and punctuation, plus we used a porter stemmer to efficiently stem every word we were dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stopwords = stopwords.words()\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "def preprocess(sentence):\n",
    "    if sentence == \"\" or isNaN(sentence):\n",
    "        return []\n",
    "    text_tokens = nltk.word_tokenize(sentence)\n",
    "    tokens_without_sw = [ps.stem(word) for word in text_tokens if not word in stopwords and word.isalnum()]\n",
    "    return tokens_without_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create an index dictionary, called preprocessed_index, save it as a pickle file, and later invert it to allow us to efficiently search through it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {}\n",
    "for idx, desc in zip(ds.index, ds.anime_description):\n",
    "    if idx % 1000 == 0: \n",
    "        print(\"---->\", idx) #this is a simple print to check the progress while building the dictionary\n",
    "    preprocessed_list = preprocess(desc)\n",
    "    for term in preprocessed_list:\n",
    "        if term in index_dict:\n",
    "            index_dict[term].append(idx)\n",
    "        else:\n",
    "            index_dict[term] = []\n",
    "a_file = open(\"preprocessed_index.pkl\", \"wb\") #we decided to save the dictionary as a pickle file\n",
    "pickle.dump(index_dict, a_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"preprocessed_index.pkl\", \"rb\")\n",
    "inverseindex_dict = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_index = {} #SHOULD I ADD THIS CELL???\n",
    "for key, value in index_dict.items():\n",
    "    for idx in value:\n",
    "        if idx in inverseindex_dict:\n",
    "            inverseindex_dict[idx].add()\n",
    "        else:\n",
    "            inverseindex_dict[idx] = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a simple example of how our code works down here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dragon\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>anime_description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9728</th>\n",
       "      <td>Chaos Dragon: Sekiryuu Seneki</td>\n",
       "      <td>In 3015, the year of Huanli, two countries, Do...</td>\n",
       "      <td>https://myanimelist.net/anime/30091/Chaos_Drag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8194</th>\n",
       "      <td>Bada-ui Jeonseol Jangbogo</td>\n",
       "      <td>In a not too distant future, the government se...</td>\n",
       "      <td>https://myanimelist.net/anime/10194/Bada-ui_Je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>X/1999</td>\n",
       "      <td>At the millennial edge, the concluding battle ...</td>\n",
       "      <td>https://myanimelist.net/anime/155/X_1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16387</th>\n",
       "      <td>Kiki to Lala no Mamatte Suteki!</td>\n",
       "      <td>Lala finds a myserious egg that hatches into a...</td>\n",
       "      <td>https://myanimelist.net/anime/29249/Kiki_to_La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>Hayate no Gotoku!!</td>\n",
       "      <td>Dr. Saotome returns from the dead after being ...</td>\n",
       "      <td>https://myanimelist.net/anime/4192/Hayate_no_G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>Fairy Tail Movie 2: Dragon Cry</td>\n",
       "      <td>Dragon Cry is a magical artifact of deadly pow...</td>\n",
       "      <td>https://myanimelist.net/anime/30778/Fairy_Tail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>Dragon Ball Kai: Mirai ni Heiwa wo! Goku no Ta...</td>\n",
       "      <td>Special episode about Trunks going back to the...</td>\n",
       "      <td>https://myanimelist.net/anime/11359/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7674</th>\n",
       "      <td>Choegang Top Plate</td>\n",
       "      <td>The storyline is unknown but judging from the ...</td>\n",
       "      <td>https://myanimelist.net/anime/31211/Choegang_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17401</th>\n",
       "      <td>Harem Time The Animation</td>\n",
       "      <td>A special governmental program has been introd...</td>\n",
       "      <td>https://myanimelist.net/anime/14991/Harem_Time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>Rozen Maiden: Meitantei Kunkun - Duell Walzer</td>\n",
       "      <td>Learning to ride and tame dragons comes easy t...</td>\n",
       "      <td>https://myanimelist.net/anime/2237/Rozen_Maide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "9728                       Chaos Dragon: Sekiryuu Seneki   \n",
       "8194                           Bada-ui Jeonseol Jangbogo   \n",
       "5635                                              X/1999   \n",
       "16387                    Kiki to Lala no Mamatte Suteki!   \n",
       "1030                                  Hayate no Gotoku!!   \n",
       "...                                                  ...   \n",
       "1528                      Fairy Tail Movie 2: Dragon Cry   \n",
       "1526   Dragon Ball Kai: Mirai ni Heiwa wo! Goku no Ta...   \n",
       "7674                                  Choegang Top Plate   \n",
       "17401                           Harem Time The Animation   \n",
       "6143       Rozen Maiden: Meitantei Kunkun - Duell Walzer   \n",
       "\n",
       "                                       anime_description  \\\n",
       "9728   In 3015, the year of Huanli, two countries, Do...   \n",
       "8194   In a not too distant future, the government se...   \n",
       "5635   At the millennial edge, the concluding battle ...   \n",
       "16387  Lala finds a myserious egg that hatches into a...   \n",
       "1030   Dr. Saotome returns from the dead after being ...   \n",
       "...                                                  ...   \n",
       "1528   Dragon Cry is a magical artifact of deadly pow...   \n",
       "1526   Special episode about Trunks going back to the...   \n",
       "7674   The storyline is unknown but judging from the ...   \n",
       "17401  A special governmental program has been introd...   \n",
       "6143   Learning to ride and tame dragons comes easy t...   \n",
       "\n",
       "                                                     url  \n",
       "9728   https://myanimelist.net/anime/30091/Chaos_Drag...  \n",
       "8194   https://myanimelist.net/anime/10194/Bada-ui_Je...  \n",
       "5635            https://myanimelist.net/anime/155/X_1999  \n",
       "16387  https://myanimelist.net/anime/29249/Kiki_to_La...  \n",
       "1030   https://myanimelist.net/anime/4192/Hayate_no_G...  \n",
       "...                                                  ...  \n",
       "1528   https://myanimelist.net/anime/30778/Fairy_Tail...  \n",
       "1526   https://myanimelist.net/anime/11359/Dragon_Bal...  \n",
       "7674   https://myanimelist.net/anime/31211/Choegang_T...  \n",
       "17401  https://myanimelist.net/anime/14991/Harem_Time...  \n",
       "6143   https://myanimelist.net/anime/2237/Rozen_Maide...  \n",
       "\n",
       "[195 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input()\n",
    "query = preprocess(query)\n",
    "s = set(map(int, np.linspace(0, 19000, 19001)))\n",
    "for q in query:\n",
    "    s = s.intersection(inverseindex_dict[q])\n",
    "ds.iloc[list(s), [-1, 9, -2]] #returning the correct columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term, desc_preprocessed):\n",
    "    return desc_preprocessed.count(term)/ len(desc_preprocessed)\n",
    "def idf(term, inverseindex_dict):\n",
    "    return np.log(len(inverseindex_dict)/len(inverseindex_dict[term]))\n",
    "def tf_idf(term, desc_preprocessed, inverseindex_dict):\n",
    "    x = tf(term, desc_preprocessed) * idf(term, inverseindex_dict)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the previously defined functions, we create an inverted index containing the TF-IDF values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_invertedindex_dict = {}\n",
    "for idx, desc in zip(ds.index, ds.anime_description):\n",
    "    if idx % 1000 == 0: \n",
    "        print(\"---->\", idx)\n",
    "    preprocessed_list = preprocess(desc)\n",
    "    for term in preprocessed_list:\n",
    "        if term in tfidf_invertedindex_dict:\n",
    "            tfidf = tf_idf(term, preprocessed_list, inverseindex_dict)\n",
    "            tfidf_invertedindex_dict[term].append((idx, tfidf))\n",
    "        else:\n",
    "            tfidf_invertedindex_dict[term] = []\n",
    "a_file = open(\"tfidf_invertindex.pkl\", \"wb\")\n",
    "pickle.dump(tfidf_invertedindex_dict, a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"tfidf_invertindex.pkl\", \"rb\")\n",
    "tfidf_invertedindex_dict = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_dict = {}\n",
    "for idx, desc in zip(ds.index, ds.anime_description):\n",
    "    if idx % 1000 == 0: print(idx)\n",
    "    desc = preprocess(desc)\n",
    "    desc_vec = {}\n",
    "    for d in desc:\n",
    "        tfidf = 0\n",
    "        for i in tfidf_invertedindex_dict[d]:\n",
    "            if idx == i[0]:\n",
    "                tfidf = i[1]\n",
    "                break\n",
    "        desc_vec[d] = tfidf \n",
    "    desc_dict[idx] = desc_vec\n",
    "desc_dict_file = open(\"desc_dict.pkl\", \"wb\")\n",
    "pickle.dump(desc_dict, desc_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_dict_file = open(\"desc_dict.pkl\", \"rb\")\n",
    "desc_dict = pickle.load(desc_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_dict(dict1, dict2):\n",
    "    s = set(dict1.keys()).intersection(set(dict2.keys()))\n",
    "    c = 0\n",
    "    norm1 = 0\n",
    "    norm2 = 0\n",
    "    for i in s:\n",
    "        c += dict1[i] * dict2[i]\n",
    "    for i in dict1.values():\n",
    "        norm1 += np.power(i, 2)\n",
    "    norm1 = np.sqrt(norm1)\n",
    "    \n",
    "    for i in dict2.values():\n",
    "        norm2 += np.power(i, 2)\n",
    "    norm2 = np.sqrt(norm2)\n",
    "    \n",
    "    return c / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our dictionary and defining a fucntion to calculate the cosine similarity, we can finally execute the input query and fetch results from our search engine. We decided to return the first 4 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super saiyan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-c07044aa0f89>:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return c / (norm1 * norm2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>anime_description</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>0.533381</td>\n",
       "      <td>Following the defeat of a great adversary, Gok...</td>\n",
       "      <td>https://myanimelist.net/anime/14837/Dragon_Bal...</td>\n",
       "      <td>Dragon Ball Z Movie 14: Kami to Kami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>0.349173</td>\n",
       "      <td>As Goku investigates the destruction of the So...</td>\n",
       "      <td>https://myanimelist.net/anime/901/Dragon_Ball_...</td>\n",
       "      <td>Dragon Ball Z Movie 08: Moetsukiro!! Nessen, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7059</th>\n",
       "      <td>0.314386</td>\n",
       "      <td>Super deformed .</td>\n",
       "      <td>https://myanimelist.net/anime/30291/Mini_Hama_...</td>\n",
       "      <td>Mini Hama: Minimum Hamatora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8786</th>\n",
       "      <td>0.296940</td>\n",
       "      <td>is a cinematic attraction at Universal Studios...</td>\n",
       "      <td>https://myanimelist.net/anime/42449/Dragon_Bal...</td>\n",
       "      <td>Dragon Ball Z: The Real 4-D at Super Tenkaichi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      similarity                                  anime_description  \\\n",
       "1972    0.533381  Following the defeat of a great adversary, Gok...   \n",
       "2248    0.349173  As Goku investigates the destruction of the So...   \n",
       "7059    0.314386                                   Super deformed .   \n",
       "8786    0.296940  is a cinematic attraction at Universal Studios...   \n",
       "\n",
       "                                                    url  \\\n",
       "1972  https://myanimelist.net/anime/14837/Dragon_Bal...   \n",
       "2248  https://myanimelist.net/anime/901/Dragon_Ball_...   \n",
       "7059  https://myanimelist.net/anime/30291/Mini_Hama_...   \n",
       "8786  https://myanimelist.net/anime/42449/Dragon_Bal...   \n",
       "\n",
       "                                                  title  \n",
       "1972               Dragon Ball Z Movie 14: Kami to Kami  \n",
       "2248  Dragon Ball Z Movie 08: Moetsukiro!! Nessen, R...  \n",
       "7059                        Mini Hama: Minimum Hamatora  \n",
       "8786  Dragon Ball Z: The Real 4-D at Super Tenkaichi...  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input()\n",
    "query = preprocess(query)\n",
    "query_vec = {}\n",
    "for q in query:\n",
    "    query_vec[q] = tf_idf(q, query, inverseindex_dict)\n",
    "sim_dict = {}\n",
    "for idx, desc_tfidf in desc_dict.items():\n",
    "    sim_dict[idx] = cosine_similarity_dict(query_vec, desc_tfidf)\n",
    "sim_ds = pd.DataFrame({'similarity': sim_dict.values()})\n",
    "ds['similarity'] = sim_dict.values()\n",
    "k = 4 #our choice of k-results\n",
    "clean_dict = {k: sim_dict[k] for k in sim_dict.keys() if sim_dict[k] != 0.0 and not np.isnan(sim_dict[k])}\n",
    "clean_dict = {k: v for k, v in sorted(clean_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "l = [i for i in clean_dict.keys()]\n",
    "l = l[-k:]\n",
    "\n",
    "heap = hq.heapify(l) #storing the top k-results in a heap\n",
    "ds.iloc[l, [-1, 9, 14, 15]].sort_values('similarity', ascending=False) \n",
    "#we print the correct columns and sort them accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative solution (way faster and simpler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We felt that using cosine similarity wasn't possibly the best approach to find results. This is our take on a simpler and, arguably, more efficient solution to find relevant results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saiyan race\n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "query = preprocess(query)\n",
    "s = set(map(int, np.linspace(0, 19000, 19001)))\n",
    "for q in query:\n",
    "    l = [x[0] for x in tfidf_invertedindex_dict[q]]\n",
    "    s = s.intersection(l)\n",
    "# with open('anime.tsv', 'rb') as anime:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_title</th>\n",
       "      <th>anime_type</th>\n",
       "      <th>anime_num_episodes</th>\n",
       "      <th>anime_aired</th>\n",
       "      <th>anime_status</th>\n",
       "      <th>anime_score</th>\n",
       "      <th>anime_users</th>\n",
       "      <th>anime_rank</th>\n",
       "      <th>anime_popularity</th>\n",
       "      <th>anime_description</th>\n",
       "      <th>anime_related</th>\n",
       "      <th>anime_characters</th>\n",
       "      <th>anime_voices</th>\n",
       "      <th>anime_staff</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Dragon Ball Super: Broly - MyAnimeList.net</td>\n",
       "      <td>Movie</td>\n",
       "      <td>1</td>\n",
       "      <td>['Dec 14, 2018']</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>8.12</td>\n",
       "      <td>278,097</td>\n",
       "      <td>#402</td>\n",
       "      <td>#631</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>['Dragon Ball Super', 'Dragon Ball Super: Supe...</td>\n",
       "      <td>[['Son', 'Gokuu'], ['Vegeta'], ['Broly'], ['Go...</td>\n",
       "      <td>[['Nozawa', 'Masako'], ['Horikawa', 'Ryo'], ['...</td>\n",
       "      <td>[[['Sabat', 'Christopher'], 'Producer, ADR Dir...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>Dragon Ball Kai (Dragon Ball Z Kai) - MyAnimeL...</td>\n",
       "      <td>TV</td>\n",
       "      <td>97</td>\n",
       "      <td>['Apr 5, 2009', 'Mar 27, 2011']</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>7.71</td>\n",
       "      <td>315,238</td>\n",
       "      <td>#1036</td>\n",
       "      <td>#542</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>['Dragon Ball Z', 'Dragon Ball', 'Dragon Ball ...</td>\n",
       "      <td>[['Son', 'Gokuu'], ['Vegeta'], ['Son', 'Gohan'...</td>\n",
       "      <td>[['Nozawa', 'Masako'], ['Horikawa', 'Ryo'], ['...</td>\n",
       "      <td>[[['Sabat', 'Christopher'], 'Producer, ADR Dir...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Special</td>\n",
       "      <td>1</td>\n",
       "      <td>['Oct 17, 1990']</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>7.55</td>\n",
       "      <td>87,557</td>\n",
       "      <td>#1469</td>\n",
       "      <td>#1785</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>['Dragon Ball: Episode of Bardock', 'Dragon Ba...</td>\n",
       "      <td>[['Freeza'], ['Bardock'], ['Son', 'Gokuu'], ['...</td>\n",
       "      <td>[['Nakao', 'Ryusei'], ['Nozawa', 'Masako'], ['...</td>\n",
       "      <td>[[['Nishio', 'Daisuke'], 'Director'], [['Koyam...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            anime_title anime_type  \\\n",
       "392          Dragon Ball Super: Broly - MyAnimeList.net      Movie   \n",
       "1014  Dragon Ball Kai (Dragon Ball Z Kai) - MyAnimeL...         TV   \n",
       "1437  Dragon Ball Z Special 1: Tatta Hitori no Saish...    Special   \n",
       "\n",
       "     anime_num_episodes                      anime_aired     anime_status  \\\n",
       "392                   1                 ['Dec 14, 2018']  Finished Airing   \n",
       "1014                 97  ['Apr 5, 2009', 'Mar 27, 2011']  Finished Airing   \n",
       "1437                  1                 ['Oct 17, 1990']  Finished Airing   \n",
       "\n",
       "      anime_score anime_users anime_rank anime_popularity  \\\n",
       "392          8.12     278,097       #402             #631   \n",
       "1014         7.71     315,238      #1036             #542   \n",
       "1437         7.55      87,557      #1469            #1785   \n",
       "\n",
       "                                      anime_description  \\\n",
       "392   Forty-one years ago on Planet Vegeta, home of ...   \n",
       "1014  Five years after the events of Dragon Ball, ma...   \n",
       "1437  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "\n",
       "                                          anime_related  \\\n",
       "392   ['Dragon Ball Super', 'Dragon Ball Super: Supe...   \n",
       "1014  ['Dragon Ball Z', 'Dragon Ball', 'Dragon Ball ...   \n",
       "1437  ['Dragon Ball: Episode of Bardock', 'Dragon Ba...   \n",
       "\n",
       "                                       anime_characters  \\\n",
       "392   [['Son', 'Gokuu'], ['Vegeta'], ['Broly'], ['Go...   \n",
       "1014  [['Son', 'Gokuu'], ['Vegeta'], ['Son', 'Gohan'...   \n",
       "1437  [['Freeza'], ['Bardock'], ['Son', 'Gokuu'], ['...   \n",
       "\n",
       "                                           anime_voices  \\\n",
       "392   [['Nozawa', 'Masako'], ['Horikawa', 'Ryo'], ['...   \n",
       "1014  [['Nozawa', 'Masako'], ['Horikawa', 'Ryo'], ['...   \n",
       "1437  [['Nakao', 'Ryusei'], ['Nozawa', 'Masako'], ['...   \n",
       "\n",
       "                                            anime_staff  \\\n",
       "392   [[['Sabat', 'Christopher'], 'Producer, ADR Dir...   \n",
       "1014  [[['Sabat', 'Christopher'], 'Producer, ADR Dir...   \n",
       "1437  [[['Nishio', 'Daisuke'], 'Director'], [['Koyam...   \n",
       "\n",
       "                                                    url  \\\n",
       "392   https://myanimelist.net/anime/36946/Dragon_Bal...   \n",
       "1014  https://myanimelist.net/anime/6033/Dragon_Ball...   \n",
       "1437  https://myanimelist.net/anime/986/Dragon_Ball_...   \n",
       "\n",
       "                                                  title  \n",
       "392                            Dragon Ball Super: Broly  \n",
       "1014                                    Dragon Ball Kai  \n",
       "1437  Dragon Ball Z Special 1: Tatta Hitori no Saish...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = [x for x in tfidf_invertedindex_dict[q] if x[0] in s]\n",
    "q = [x[0] for x in sorted(q, key=lambda x: x[1])]\n",
    "k = int(input())\n",
    "ds.iloc[q[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to define a new score that builds and improves upon the previously used cosine similarity: we assign different weights to a series of parameters, namely the cosine similarity between the query and the documents, the anime's type (TV, Movie, Music, etc.), the anime's characters and the staff that worked on it. \n",
    "The formula we used looks like this:\n",
    "\n",
    "$$Custom score =  cos.similarity * 0.25 + type * 0.25 + characters * 0.25 + staff * 0.25$$\n",
    "\n",
    "\n",
    "Type, characters and staff variables are simply calculated as 1 if any word in the query input matches a word in the respective column, and 0 if no words from the query input are found. We felt that this would be a good way to let users filter for what they are interested in in their search: for example, if I want to find a movie from the Dragon Ball serie, I just have to type \"Dragon Ball Movie\" and the animes classified as movies will be prioritized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we create the new dictionaries that will help us calculate our custom score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx_desc = {}\n",
    "inverted_idx_type = {}\n",
    "inverted_idx_chars = {}\n",
    "inverted_idx_staff = {}\n",
    "for idx, t, chars, staff, desc in zip(ds.index, ds.anime_type, ds.anime_characters, ds.anime_staff, ds.anime_description):\n",
    "    if idx % 1000 == 0: \n",
    "        print(\"---->\", idx)\n",
    "    preprocessed_list = preprocess(desc)\n",
    "    for term in preprocessed_list:\n",
    "        if term in inverted_idx_desc:\n",
    "            tfidf = tf_idf(term, preprocessed_list, inverseindex_dict)\n",
    "            inverted_idx_desc[term].append((idx, tfidf))\n",
    "        else:\n",
    "            inverted_idx_desc[term] = []\n",
    "    \n",
    "    if t in inverted_idx_type:\n",
    "            inverted_idx_type[t].append(idx)\n",
    "    else:\n",
    "        inverted_idx_type[t] = []\n",
    "            \n",
    "    for char_name in chars:\n",
    "        if char_name in inverted_idx_chars:\n",
    "            inverted_idx_chars[char_name].append(idx)\n",
    "        else:\n",
    "            inverted_idx_chars[char_name] = []\n",
    "        \n",
    "    for staff_name in staff:\n",
    "        if staff_name in inverted_idx_staff:\n",
    "            inverted_idx_staff[staff_name].append(idx)\n",
    "        else:\n",
    "            inverted_idx_staff[staff_name] = []   \n",
    "    \n",
    "desc_file = open(\"invertindex_desc.pkl\", \"wb\")\n",
    "pickle.dump(inverted_idx_desc, desc_file)\n",
    "\n",
    "type_file = open(\"invertindex_type.pkl\", \"wb\")\n",
    "pickle.dump(inverted_idx_type, type_file)\n",
    "\n",
    "chars_file = open(\"invertindex_chars.pkl\", \"wb\")\n",
    "pickle.dump(inverted_idx_chars, chars_file)\n",
    "\n",
    "staff_file = open(\"invertindex_staff.pkl\", \"wb\")\n",
    "pickle.dump(inverted_idx_staff, staff_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_file = open(\"invertindex_desc.pkl\", \"rb\")\n",
    "inverted_idx_desc = pickle.load(desc_file)\n",
    "\n",
    "type_file = open(\"invertindex_type.pkl\", \"rb\")\n",
    "inverted_idx_type = pickle.load(type_file)\n",
    "\n",
    "chars_file = open(\"invertindex_chars.pkl\", \"rb\")\n",
    "inverted_idx_chars = pickle.load(chars_file)\n",
    "\n",
    "staff_file = open(\"invertindex_staff.pkl\", \"rb\")\n",
    "inverted_idx_staff = pickle.load(staff_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_dict_file = open(\"desc_dict.pkl\", \"rb\")\n",
    "desc_dict = pickle.load(desc_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, t, chars, staff, desc in zip(ds.index, ds.anime_type, ds.anime_characters, ds.anime_staff, ds.anime_description):\n",
    "    if idx % 1000 == 0: print(idx)\n",
    "    desc = preprocess(desc)\n",
    "    desc_vec = {}\n",
    "    for d in desc:\n",
    "        tfidf = 0\n",
    "        for i in tfidf_invertedindex_dict[d]:\n",
    "            if idx == i[0]:\n",
    "                tfidf = i[1]\n",
    "                break\n",
    "        desc_vec[d] = tfidf \n",
    "    desc_dict[idx] = {'desc': desc_vec, 'chars': chars, 'type': t, 'staff': staff}\n",
    "big_dict_file = open(\"big_dict.pkl\", \"wb\")\n",
    "pickle.dump(desc_dict, big_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dict_file = open(\"big_dict.pkl\", \"rb\")\n",
    "big_dict = pickle.load(big_dict_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can input a query and try out our new score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saiyan race movie goku\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-c07044aa0f89>:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return c / (norm1 * norm2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>anime_description</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>0.099594</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>0.093649</td>\n",
       "      <td>One peaceful afternoon, the Son family and fri...</td>\n",
       "      <td>https://myanimelist.net/anime/22695/Dragon_Bal...</td>\n",
       "      <td>Dragon Ball Z: Summer Vacation Special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>0.090904</td>\n",
       "      <td>Specials included with the original series.\\n\\...</td>\n",
       "      <td>https://myanimelist.net/anime/2520/Dragon_Ball...</td>\n",
       "      <td>Dragon Ball Specials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158</th>\n",
       "      <td>0.089819</td>\n",
       "      <td>A retelling of Dragon Ball's origin with a dif...</td>\n",
       "      <td>https://myanimelist.net/anime/2748/Dash_Kappei</td>\n",
       "      <td>Dash! Kappei</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      similarity                                  anime_description  \\\n",
       "1437    0.099594  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "5547    0.093649  One peaceful afternoon, the Son family and fri...   \n",
       "4890    0.090904  Specials included with the original series.\\n\\...   \n",
       "3158    0.089819  A retelling of Dragon Ball's origin with a dif...   \n",
       "\n",
       "                                                    url  \\\n",
       "1437  https://myanimelist.net/anime/986/Dragon_Ball_...   \n",
       "5547  https://myanimelist.net/anime/22695/Dragon_Bal...   \n",
       "4890  https://myanimelist.net/anime/2520/Dragon_Ball...   \n",
       "3158     https://myanimelist.net/anime/2748/Dash_Kappei   \n",
       "\n",
       "                                                  title  \n",
       "1437  Dragon Ball Z Special 1: Tatta Hitori no Saish...  \n",
       "5547             Dragon Ball Z: Summer Vacation Special  \n",
       "4890                               Dragon Ball Specials  \n",
       "3158                                       Dash! Kappei  "
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input()\n",
    "query = preprocess(query)\n",
    "query_vec = {}\n",
    "for q in query:\n",
    "    query_vec[q] = tf_idf(q, query, inverseindex_dict)\n",
    "sim_dict = {}\n",
    "for idx, d in big_dict.items():\n",
    "    t, chars, staff, desc = [d['type'], d['chars'], d['staff'], d['desc']]\n",
    "    t = str(t)\n",
    "    t = t.lower()\n",
    "    \n",
    "    #Searching if words contained in the query are also contained in type, characters or staff\n",
    "    t_num = 0 \n",
    "    if t in q:\n",
    "        t_num = 1\n",
    "    c_num = 0\n",
    "    for c in chars:\n",
    "        for q in query:\n",
    "            if c == q:\n",
    "                c_num += 1\n",
    "    s_num = 0\n",
    "    for s in chars:\n",
    "        for q in query:\n",
    "            if s == q:\n",
    "                s_num += 1\n",
    "                \n",
    "    #Our formula:\n",
    "    sim_dict[idx] = cosine_similarity_dict(query_vec, desc) * 0.25 +  t_num * 0.25 + c_num * 0.25 + s_num * 0.25\n",
    "    \n",
    "sim_ds = pd.DataFrame({'similarity': sim_dict.values()})\n",
    "ds['similarity'] = sim_dict.values()\n",
    "k = 4\n",
    "clean_dict = {k: sim_dict[k] for k in sim_dict.keys() if sim_dict[k] != 0.0 and not np.isnan(sim_dict[k])}\n",
    "clean_dict = {k: v for k, v in sorted(clean_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "l = [i for i in clean_dict.keys()]\n",
    "l = l[-k:]\n",
    "\n",
    "heap = hq.heapify(l)\n",
    "ds.iloc[l, [-1, 9, 14, 15]].sort_values('similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to find a movie related to the sayian race that starred Goku as a character. The results seem acceptable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BONUS: Understanding the anime's reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by parsing again the html, this time considering the reviews \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_parser2(article_path, soup):\n",
    "        anime_title = soup.title.text.strip()\n",
    "        anime_type = \"\"\n",
    "        try:\n",
    "            anime_type = getText(soup.find('span', text=\"Type:\").parent.a)\n",
    "        except:\n",
    "            pass\n",
    "        anime_num_episodes = \"\"\n",
    "        try:\n",
    "            anime_num_episodes = getText(soup.find('span', text=\"Episodes:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_aired = \"\"\n",
    "        try:\n",
    "            anime_aired = getText(soup.find('span', text=\"Aired:\").parent).split(\" to \")\n",
    "        except:\n",
    "            pass\n",
    "        anime_status = \"\"\n",
    "        try:\n",
    "            anime_status = getText(soup.find('span', text=\"Status:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_score = \"\"\n",
    "        try:\n",
    "            anime_score = getText(soup.find('span', text=\"Score:\").parent.find_all('span', {'class': 'score-label'})[0])\n",
    "        except:\n",
    "            pass\n",
    "        anime_users = \"\"\n",
    "        try:\n",
    "            anime_users = getText(soup.find('span', text=\"Members:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_rank = \"\"\n",
    "        try:\n",
    "            anime_rank = getText(soup.find('span', text=\"Ranked:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_popularity = \"\"\n",
    "        try:\n",
    "            anime_popularity = getText(soup.find('span', text=\"Popularity:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_description = \"\"\n",
    "        try:\n",
    "            anime_description = getText(soup.find('h2', text=\"Synopsis\").parent.parent.p)\n",
    "        except:\n",
    "            pass\n",
    "        anime_related = []\n",
    "        try: \n",
    "            anime_related = list(set(map(getText,soup.find('h2', text=\"Related Anime\").parent.parent.tr.find_all(lambda tag:tag.name == \"a\" and tag.href != \"\")))) \n",
    "        except: \n",
    "            pass\n",
    "        anime_characters = []\n",
    "        try:\n",
    "            anime_characters = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[1].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "            anime_characters = [e.split(\", \") for e in anime_characters]\n",
    "        except:\n",
    "            pass\n",
    "        anime_voices = []\n",
    "        try:\n",
    "            anime_voices = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[0].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "            anime_voices = [e.split(\", \") for e in anime_voices]\n",
    "        except:\n",
    "            pass\n",
    "        anime_staff = []\n",
    "        try:\n",
    "            anime_staff = list(map(lambda tr: [re.split(\"\\n+\", tr.text.strip())[0].split(', '), re.split(\"\\n+\", tr.text.strip())[1]], soup.find('h2', text=\"Staff\").find_next('div').find_all('tr')))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            anime_review = [x.strip() for x in re.split('(<br>|\\n+)', soup.find('a', text=\"More reviews\").find_next('div').text) if len(x) > 100]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        l = [anime_title,\n",
    "                anime_type,\n",
    "                anime_num_episodes,\n",
    "                anime_aired,\n",
    "                anime_status,\n",
    "                anime_score,\n",
    "                anime_users,\n",
    "                anime_rank,\n",
    "                anime_popularity,\n",
    "                anime_description,\n",
    "                anime_related,\n",
    "                anime_characters,\n",
    "                anime_voices,\n",
    "                anime_staff,\n",
    "                anime_review]\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_docs = 19125\n",
    "i = 1\n",
    "with open(\"anime.csv\", 'rb') as urls:\n",
    "    urls_ds = pd.read_csv(urls)\n",
    "    with open(\"anime_review2.tsv\", 'w', encoding=\"utf-8\", newline='') as anime:\n",
    "        for p in range(1, num_of_pages + 1):\n",
    "            flag = True\n",
    "            if (flag): \n",
    "                article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "                while os.path.isfile(article_path):\n",
    "                    article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "                    if os.path.isfile(article_path):\n",
    "                        if i % 100 == 0:\n",
    "                            print(p, \"--->\", i, \"--->\", article_path)\n",
    "                        with open(article_path, 'rb') as html:\n",
    "                            soup = BeautifulSoup(html, \"html.parser\")\n",
    "                            l = review_parser2(article_path, soup)\n",
    "                            l.append(urls_ds.url[i - 1])\n",
    "                            l.append(urls_ds.name[i - 1])\n",
    "                            writer = csv.writer(anime, delimiter='\\t')\n",
    "                            writer.writerow(l)\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCLAIMER: We actually made a mistake here, as while parsing we only gathered one review for each anime. This probably brings innacurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we add columns to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18741"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_ds2 = pd.read_csv(\"anime_review2.tsv\", delimiter='\\t')\n",
    "\n",
    "review_ds2.columns = ['anime_title',\n",
    "                'anime_type',\n",
    "                'anime_num_episodes',\n",
    "                'anime_aired',\n",
    "                'anime_status',\n",
    "                'anime_score',\n",
    "                'anime_users',\n",
    "                'anime_rank',\n",
    "                'anime_popularity',\n",
    "                'anime_description',\n",
    "                'anime_related',\n",
    "                'anime_characters',\n",
    "                'anime_voices',\n",
    "                'anime_staff',\n",
    "                'review',\n",
    "                'url',\n",
    "                'title']\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only consider the first three sentence of a review. To do so, we check the first three stopwords in a review, and discard everything that comes after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_ds2['review_short'] = review_ds2['review'].apply(lambda x: x.split(\".\")[:3])\n",
    "review_ds2['review_short_string'] = [''.join(map(str, l)) for l in review_ds['anime_review_short']]\n",
    "review_ds2.drop(columns=['review', 'review_short'])\n",
    "review_ds2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate if a review is positive or not, we use the Vader module from NLTK and its polarity scores measure, which can roughly check if a review is positive or not. The polarity score returns three values, 'negative', 'neutral', and 'positive', referring to the analyzed text. We then use the compound function to normalize the three scores into a single value. The compound score is contained between -1 and 1: -1 means that the review was absolutely negative, 0 that the review was neutral, 1 that it was absolutely positive.\n",
    "\n",
    "IMPORTANT: We didn't preprocess the reviews since Vader actually takes into consideration capitalization and exclamation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_ds2['scores'] = review_ds2['review_short_string'].apply(lambda review: vader.polarity_scores(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_ds2['compound'] = review_ds2['scores'].apply(lambda score_dict: score_dict['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What Anime Type are the most popular based on your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_df = review_ds2.groupby('anime_type').mean()\n",
    "type_df_sorted = type_df.sort_values(by = ['compound'], ascending=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anime_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TV</th>\n",
       "      <td>0.212936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Special</th>\n",
       "      <td>0.192303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OVA</th>\n",
       "      <td>0.152073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movie</th>\n",
       "      <td>0.142555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ONA</th>\n",
       "      <td>0.104817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            compound\n",
       "anime_type          \n",
       "TV          0.212936\n",
       "Special     0.192303\n",
       "OVA         0.152073\n",
       "Movie       0.142555\n",
       "ONA         0.104817"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_df_sorted.drop('anime_score', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that from our analysis the animes classified as Tv shows are (marginally) the ones with the most positive reviews, quickly followed by the \"special\" category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Does the number of episodes has any impact on the sentiment of the users?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check with a simple Pearson coefficient if we can find a correlation between the number of episodes and the sentiment of the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_corr_new = review_ds2[['compound', 'anime_num_episodes']]\n",
    "c = new_df_corr_new[(new_df_corr_new.compound != 0) & (new_df_corr_new.anime_num_episodes != 'Unknown')]\n",
    "#we filtered for the compound values different than 0, since if an anime doesn't have any review it will\n",
    "#display 0 as a compound score and that would skew the results given the amount of animes without reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None #to avoid errors when converting to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['anime_num_episodes_int'] = c['anime_num_episodes'].astype(int) #converting to the correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028011316551619578"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = pearsonr(c['anime_num_episodes_int'], c['compound'])\n",
    "coeff[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no correlation. This felt strange to us and it may be that our analysis was flawed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is your sentiment analysis in line with the scores of the animes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In similar fashion, we repeat the steps we did earlier and check for a correlation between the score of an anime and its compound score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = review_ds2[['compound', 'anime_score']]\n",
    "e = review_ds2[(d.compound != 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = e.dropna(axis= 0, how= 'any');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10796921928967673"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = pearsonr(f['anime_score'], f['compound'])\n",
    "coeff[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, no correlation. We feel this shouldn't actually be the case and the results are probably wrong due to the fact that we didn't parse the reviews in the intended way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question\n",
    "\n",
    "In this problem, the goal is to maximize the sum of values contained in a list, where each value represents the time that the person who made the appointment wants to spend with the personal trainer, so you want to provide the personal trainer with a program that maximizes the total length of accepted appointments. The important condition is that the personal trainer needs a break between appointments and cannot accept two consecutive bookings. In addition, reservations are considered in the order in which they were made, so that the personal trainer cannot go back and accept past bookings, but must follow the chronological order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Formalization of the problem**\n",
    "From an algorithmic point of view, the problem results in: given an array of positive numbers, find the maximum sum of a subsequence with the constraint that no two numbers in the sequence should be adjacent in the array.\n",
    "\n",
    "### **Alghorithmic Idea**\n",
    "The time complexity of this solution is $O(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PseudoCode**\n",
    "\n",
    "\tInput:\n",
    "  \t  Arr = array of positive numbers with length n.\n",
    "\n",
    "\tFunction trainer(Arr):\n",
    "\t\tn = length of Arr.\n",
    "\t\tBase Cases\n",
    "\t\tIn case there are no bookings, no operations are carried out.\n",
    "\t\treturn 0\n",
    "\t\tIn case there is only one booking, only this one is chosen.\n",
    "\t\treturn Arr[0]\n",
    "\n",
    "\t\tAdvanced Case:\n",
    "\t\tDefine an empty list and Start for cycle.\n",
    "\t\n",
    "\t\tStart while cycle.\n",
    "\t\tExtract the indices of the numbers that composed the largest sum.\n",
    "\n",
    "\t\tPrint the sub-list that generates the maximum sum and the maximum\n",
    "\t\tcorresponding sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2** \n",
    "Implement a program that given in input an instance in the form given above, gives the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(Arr):\n",
    "    n = len(Arr)\n",
    "    if not Arr:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return Arr[0]\n",
    "\n",
    "    new_list = [None for _ in range(n + 1)]\n",
    "    new_list[0] = 0\n",
    "    new_list[1] = Arr[0]\n",
    "\n",
    "\n",
    "    for i in range(2, n + 1):\n",
    "        new_list[i] = max(new_list[i - 2] + Arr[i - 1], new_list[i - 1])\n",
    "\n",
    "    res = []\n",
    "    i = n\n",
    "    while i > 0:\n",
    "        if i > 1:\n",
    "            if new_list[i - 1] > new_list[i - 2] + Arr[i - 1]:\n",
    "                i -=  1\n",
    "            else:\n",
    "                res.append(Arr[i - 1])\n",
    "                i -=  2\n",
    "        elif i == 1 and new_list[i - 1] < Arr[i - 1]:\n",
    "            res.append(Arr[i - 1])\n",
    "            i -= 1\n",
    "        else:\n",
    "            i -= 1\n",
    "    res.reverse()\n",
    "    return res, sum(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The appointment list is: [30, 40, 25, 50, 30, 20] \n",
      "\n",
      "The best way to accept appointements is: [40, 50, 20] with max duration of: 110\n"
     ]
    }
   ],
   "source": [
    "## Test 01\n",
    "Arr = [30, 40, 25, 50, 30, 20]\n",
    "print('The appointment list is:', Arr, '\\n') \n",
    "print('The best way to accept appointements is:', trainer(Arr)[0], 'with max duration of:', trainer(Arr)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The appointment list is: [30, 40, 25, 10, 20, 50, 30, 20] \n",
      "\n",
      "The best way to accept appointements is: [30, 25, 50, 20] with max duration of: 125\n"
     ]
    }
   ],
   "source": [
    "## Test 02\n",
    "Arr = [30, 40, 25, 10, 20, 50, 30, 20]\n",
    "print('The appointment list is:', Arr, '\\n') \n",
    "print('The best way to accept appointements is:', trainer(Arr)[0], 'with max duration of:', trainer(Arr)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The appointment list is: [79, 112, 28, 33, 117, 30, 41, 23, 7, 58, 113, 12, 78, 43, 92, 75, 13, 98, 83, 102, 1, 15, 52, 31, 88, 71, 70, 3, 66, 26, 96, 20, 47, 36, 110, 62, 18, 93, 67, 84, 48, 6, 10, 46, 37, 72, 34, 51, 89, 40] \n",
      "\n",
      "The best way to accept appointements is: [112, 117, 41, 7, 113, 78, 92, 98, 102, 52, 88, 70, 66, 96, 47, 110, 93, 84, 6, 46, 72, 51, 40] with max duration of: 1681\n"
     ]
    }
   ],
   "source": [
    "## Test 03 (n = 50)\n",
    "Arr = [79, 112, 28, 33, 117, 30, 41, 23, 7, 58, 113, 12, 78, 43, 92, 75, 13, 98, 83, 102, 1, 15, 52, 31, 88, 71, 70, 3, 66, 26, 96, 20, 47, 36, 110, 62, 18, 93, 67, 84, 48, 6,\n",
    "       10, 46, 37, 72, 34, 51, 89, 40]\n",
    "print('The appointment list is:', Arr, '\\n') \n",
    "print('The best way to accept appointements is:', trainer(Arr)[0], 'with max duration of:', trainer(Arr)[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "homework3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
