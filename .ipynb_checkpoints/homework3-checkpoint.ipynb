{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Parsa33033/ADM_HW3/blob/main/homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR-dxW-68OXP",
    "outputId": "f197a5c5-f6a8-4303-a0dd-7fd5215e53cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rkVdjQff8ggD"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5F7tX0Mi81Ir"
   },
   "outputs": [],
   "source": [
    "num_of_pages = 384\n",
    "directory = \"\"\n",
    "links_file = \"anime.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXKNsnC682xk",
    "outputId": "a9369342-4bc3-41aa-e9b8-d40eb636b0d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 384/384 [02:53<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "anime = []\n",
    "\n",
    "                               \n",
    "if not os.path.isfile(links_file):\n",
    "    with open(directory + links_file, \"w\", encoding='utf-8', newline='') as fobj:\n",
    "        fobj.write(\"page,name,url\\n\")\n",
    "        for page in tqdm(range(0, num_of_pages)):\n",
    "            url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "            response = requests.get(url)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for tag in soup.find_all('tr'):\n",
    "                links = tag.find_all('a')\n",
    "                for link in links:        \n",
    "                    if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                        x = (page + 1, link.contents[0], link.get('href'))\n",
    "                        anime.append(x)\n",
    "                        writer = csv.writer(fobj)\n",
    "                        writer.writerow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_dir = directory + \"pages\"\n",
    "page_dir = pages_dir + \"/\" + \"page\"\n",
    "article = \"article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "ZYKNkYqf84p_",
    "outputId": "82b6bd03-2088-4b25-da8c-152586d6766b"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(pages_dir):\n",
    "    os.makedirs(pages_dir)\n",
    "    \n",
    "def urlEncodeNonAscii(b):\n",
    "    return re.sub('[\\x80-\\xFF]', lambda c: '%%%02x' % ord(c.group(0)), b)\n",
    "\n",
    "ds = pd.read_csv(directory + links_file)\n",
    "i = 1\n",
    "for p, u in tqdm(zip(ds.page, ds.url)):\n",
    "    d = page_dir + str(p)\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "    d = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "    \n",
    "    if not os.path.exists(d):\n",
    "#         request = requests.get(u, headers={'Cache-Control': 'no-cache', 'User-Agent': 'Mozilla/5.0'})\n",
    "        req = Request(u, headers={'User-Agent': 'XYZ/3.0'})\n",
    "        webpage = urlopen(req, timeout=100).read()\n",
    "        with open(d, \"w\", encoding='utf-8') as fobj:\n",
    "            fobj.write(webpage)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9WbbqyPD__n"
   },
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(parent):\n",
    "    return ''.join(parent.find_all(text=True, recursive=False)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_parser(article_path, soup):\n",
    "        anime_title = soup.title.text.strip()\n",
    "        anime_type = \"\"\n",
    "        try:\n",
    "            anime_type = getText(soup.find('span', text=\"Type:\").parent.a)\n",
    "        except:\n",
    "            pass\n",
    "        anime_num_episodes = \"\"\n",
    "        try:\n",
    "            anime_num_episodes = getText(soup.find('span', text=\"Episodes:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_aired = \"\"\n",
    "        try:\n",
    "            anime_aired = getText(soup.find('span', text=\"Aired:\").parent).split(\" to \")\n",
    "        except:\n",
    "            pass\n",
    "        anime_status = \"\"\n",
    "        try:\n",
    "            anime_status = getText(soup.find('span', text=\"Status:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_score = \"\"\n",
    "        try:\n",
    "            anime_score = getText(soup.find('span', text=\"Score:\").parent.find_all('span', {'class': 'score-label'})[0])\n",
    "        except:\n",
    "            pass\n",
    "        anime_users = \"\"\n",
    "        try:\n",
    "            anime_users = getText(soup.find('span', text=\"Members:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_rank = \"\"\n",
    "        try:\n",
    "            anime_rank = getText(soup.find('span', text=\"Ranked:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_popularity = \"\"\n",
    "        try:\n",
    "            anime_popularity = getText(soup.find('span', text=\"Popularity:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_description = \"\"\n",
    "        try:\n",
    "            anime_description = getText(soup.find('h2', text=\"Synopsis\").parent.parent.p)\n",
    "        except:\n",
    "            pass\n",
    "        anime_related = []\n",
    "        try: \n",
    "            anime_related = list(set(map(getText,soup.find('h2', text=\"Related Anime\").parent.parent.tr.find_all(lambda tag:tag.name == \"a\" and tag.href != \"\")))) \n",
    "        except: \n",
    "            pass\n",
    "        anime_characters = []\n",
    "        try:\n",
    "            anime_characters = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[1].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "        except:\n",
    "            pass\n",
    "        anime_voices = []\n",
    "        try:\n",
    "            anime_voices = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[0].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "        except:\n",
    "            pass\n",
    "        anime_staff = []\n",
    "        try:\n",
    "            anime_staff = list(map(lambda tr: re.split(\"\\n+\", tr.text.strip()), soup.find('h2', text=\"Staff\").find_next('div').find_all('tr')))\n",
    "        except:\n",
    "            pass\n",
    "        l = [anime_title,\n",
    "                anime_type,\n",
    "                anime_num_episodes,\n",
    "                anime_aired,\n",
    "                anime_status,\n",
    "                anime_score,\n",
    "                anime_users,\n",
    "                anime_rank,\n",
    "                anime_popularity,\n",
    "                anime_description,\n",
    "                anime_related,\n",
    "                anime_characters,\n",
    "                anime_voices,\n",
    "                anime_staff]\n",
    "        return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ---> 100 ---> pages/page2/article_100.html\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0e8aa40ecc04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'anime.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0manime\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-54d7342f7960>\u001b[0m in \u001b[0;36marticle_parser\u001b[1;34m(article_path, soup)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0marticle_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m         \u001b[0manime_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0manime_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Type:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0manime_num_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Episodes:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0manime_aired\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Aired:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" to \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ddd33070def8>\u001b[0m in \u001b[0;36mgetText\u001b[1;34m(parent)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "num_of_docs = 19125\n",
    "i = 1\n",
    "for p in range(1, num_of_pages + 1):\n",
    "    flag = True\n",
    "    if (flag): \n",
    "        article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "    while os.path.isfile(article_path):\n",
    "        article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "        if os.path.isfile(article_path):\n",
    "            if i % 100 == 0:\n",
    "                print(p, \"--->\", i, \"--->\", article_path)\n",
    "                break\n",
    "            with open(article_path, 'rb') as html:\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                l = article_parser(article_path, soup)\n",
    "                with open('anime.tsv', 'w', encoding=\"utf-8\") as anime:\n",
    "                    writer = csv.writer(anime, delimiter='\\t')\n",
    "                    writer.writerow(l)\n",
    "        i += 1\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "homework3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
