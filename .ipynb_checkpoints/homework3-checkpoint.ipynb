{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Parsa33033/ADM_HW3/blob/main/homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR-dxW-68OXP",
    "outputId": "f197a5c5-f6a8-4303-a0dd-7fd5215e53cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "rkVdjQff8ggD"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "import heapq as hq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "5F7tX0Mi81Ir"
   },
   "outputs": [],
   "source": [
    "num_of_pages = 384\n",
    "directory = \"\"\n",
    "links_file = \"anime.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXKNsnC682xk",
    "outputId": "a9369342-4bc3-41aa-e9b8-d40eb636b0d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 384/384 [02:53<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "anime = []\n",
    "\n",
    "                               \n",
    "if not os.path.isfile(links_file):\n",
    "    with open(directory + links_file, \"w\", encoding='utf-8', newline='') as fobj:\n",
    "        fobj.write(\"page,name,url\\n\")\n",
    "        for page in tqdm(range(0, num_of_pages)):\n",
    "            url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "            response = requests.get(url)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for tag in soup.find_all('tr'):\n",
    "                links = tag.find_all('a')\n",
    "                for link in links:        \n",
    "                    if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                        x = (page + 1, link.contents[0], link.get('href'))\n",
    "                        anime.append(x)\n",
    "                        writer = csv.writer(fobj)\n",
    "                        writer.writerow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_dir = directory + \"pages\"\n",
    "page_dir = pages_dir + \"/\" + \"page\"\n",
    "article = \"article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "ZYKNkYqf84p_",
    "outputId": "82b6bd03-2088-4b25-da8c-152586d6766b"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(pages_dir):\n",
    "    os.makedirs(pages_dir)\n",
    "    \n",
    "def urlEncodeNonAscii(b):\n",
    "    return re.sub('[\\x80-\\xFF]', lambda c: '%%%02x' % ord(c.group(0)), b)\n",
    "\n",
    "ds = pd.read_csv(directory + links_file)\n",
    "i = 1\n",
    "for p, u in tqdm(zip(ds.page, ds.url)):\n",
    "    d = page_dir + str(p)\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "    d = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "    \n",
    "    if not os.path.exists(d):\n",
    "#         request = requests.get(u, headers={'Cache-Control': 'no-cache', 'User-Agent': 'Mozilla/5.0'})\n",
    "        req = Request(u, headers={'User-Agent': 'XYZ/3.0'})\n",
    "        webpage = urlopen(req, timeout=100).read()\n",
    "        with open(d, \"w\", encoding='utf-8') as fobj:\n",
    "            fobj.write(webpage)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9WbbqyPD__n"
   },
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(parent):\n",
    "    return ''.join(parent.find_all(text=True, recursive=False)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_parser(article_path, soup):\n",
    "        anime_title = soup.title.text.strip()\n",
    "        anime_type = \"\"\n",
    "        try:\n",
    "            anime_type = getText(soup.find('span', text=\"Type:\").parent.a)\n",
    "        except:\n",
    "            pass\n",
    "        anime_num_episodes = \"\"\n",
    "        try:\n",
    "            anime_num_episodes = getText(soup.find('span', text=\"Episodes:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_aired = \"\"\n",
    "        try:\n",
    "            anime_aired = getText(soup.find('span', text=\"Aired:\").parent).split(\" to \")\n",
    "        except:\n",
    "            pass\n",
    "        anime_status = \"\"\n",
    "        try:\n",
    "            anime_status = getText(soup.find('span', text=\"Status:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_score = \"\"\n",
    "        try:\n",
    "            anime_score = getText(soup.find('span', text=\"Score:\").parent.find_all('span', {'class': 'score-label'})[0])\n",
    "        except:\n",
    "            pass\n",
    "        anime_users = \"\"\n",
    "        try:\n",
    "            anime_users = getText(soup.find('span', text=\"Members:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_rank = \"\"\n",
    "        try:\n",
    "            anime_rank = getText(soup.find('span', text=\"Ranked:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_popularity = \"\"\n",
    "        try:\n",
    "            anime_popularity = getText(soup.find('span', text=\"Popularity:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_description = \"\"\n",
    "        try:\n",
    "            anime_description = getText(soup.find('h2', text=\"Synopsis\").parent.parent.p)\n",
    "        except:\n",
    "            pass\n",
    "        anime_related = []\n",
    "        try: \n",
    "            anime_related = list(set(map(getText,soup.find('h2', text=\"Related Anime\").parent.parent.tr.find_all(lambda tag:tag.name == \"a\" and tag.href != \"\")))) \n",
    "        except: \n",
    "            pass\n",
    "        anime_characters = []\n",
    "        try:\n",
    "            anime_characters = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[1].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "            anime_characters = [e.split(\", \") for e in anime_characters]\n",
    "        except:\n",
    "            pass\n",
    "        anime_voices = []\n",
    "        try:\n",
    "            anime_voices = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[0].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "            anime_voices = [e.split(\", \") for e in anime_voices]\n",
    "        except:\n",
    "            pass\n",
    "        anime_staff = []\n",
    "        try:\n",
    "            anime_staff = list(map(lambda tr: [re.split(\"\\n+\", tr.text.strip())[0].split(', '), re.split(\"\\n+\", tr.text.strip())[1]], soup.find('h2', text=\"Staff\").find_next('div').find_all('tr')))\n",
    "        except:\n",
    "            pass\n",
    "        l = [anime_title,\n",
    "                anime_type,\n",
    "                anime_num_episodes,\n",
    "                anime_aired,\n",
    "                anime_status,\n",
    "                anime_score,\n",
    "                anime_users,\n",
    "                anime_rank,\n",
    "                anime_popularity,\n",
    "                anime_description,\n",
    "                anime_related,\n",
    "                anime_characters,\n",
    "                anime_voices,\n",
    "                anime_staff]\n",
    "        return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_docs = 19125\n",
    "i = 1\n",
    "with open('anime.csv', 'rb') as urls:\n",
    "    urls_ds = pd.read_csv(urls)\n",
    "    with open('anime.tsv', 'w', encoding=\"utf-8\", newline='') as anime:\n",
    "        for p in range(1, num_of_pages + 1):\n",
    "            flag = True\n",
    "            if (flag): \n",
    "                article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "                while os.path.isfile(article_path):\n",
    "                    article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "                    if os.path.isfile(article_path):\n",
    "                        if i % 100 == 0:\n",
    "                            print(p, \"--->\", i, \"--->\", article_path)\n",
    "                        with open(article_path, 'rb') as html:\n",
    "                            soup = BeautifulSoup(html, \"html.parser\")\n",
    "                            l = article_parser(article_path, soup)\n",
    "                            l.append(urls_ds.url[i - 1])\n",
    "                            l.append(urls_ds.name[i - 1])\n",
    "                            writer = csv.writer(anime, delimiter='\\t')\n",
    "                            writer.writerow(l)\n",
    "                    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18741"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv(\"anime.tsv\", delimiter='\\t')\n",
    "ds.columns = ['anime_title',\n",
    "                'anime_type',\n",
    "                'anime_num_episodes',\n",
    "                'anime_aired',\n",
    "                'anime_status',\n",
    "                'anime_score',\n",
    "                'anime_users',\n",
    "                'anime_rank',\n",
    "                'anime_popularity',\n",
    "                'anime_description',\n",
    "                'anime_related',\n",
    "                'anime_characters',\n",
    "                'anime_voices',\n",
    "                'anime_staff',\n",
    "                'url',\n",
    "                'title']\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stopwords = stopwords.words()\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "def preprocess(sentence):\n",
    "    if sentence == \"\" or isNaN(sentence):\n",
    "        return []\n",
    "    text_tokens = nltk.word_tokenize(sentence)\n",
    "    tokens_without_sw = [ps.stem(word) for word in text_tokens if not word in stopwords and word.isalnum()]\n",
    "    return tokens_without_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> 0\n",
      "----> 1000\n",
      "----> 2000\n",
      "----> 3000\n",
      "----> 4000\n",
      "----> 5000\n",
      "----> 6000\n",
      "----> 7000\n",
      "----> 8000\n",
      "----> 9000\n",
      "----> 10000\n",
      "----> 11000\n",
      "----> 12000\n",
      "----> 13000\n",
      "----> 14000\n",
      "----> 15000\n",
      "----> 16000\n",
      "----> 17000\n",
      "----> 18000\n"
     ]
    }
   ],
   "source": [
    "index_dict = {}\n",
    "for idx, desc in zip(ds.index, ds.anime_description):\n",
    "    if idx % 1000 == 0: \n",
    "        print(\"---->\", idx)\n",
    "    preprocessed_list = preprocess(desc)\n",
    "    for term in preprocessed_list:\n",
    "        if term in index_dict:\n",
    "            index_dict[term].append(idx)\n",
    "        else:\n",
    "            index_dict[term] = []\n",
    "a_file = open(\"preprocessed_index.pkl\", \"wb\")\n",
    "pickle.dump(index_dict, a_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"preprocessed_index.pkl\", \"rb\")\n",
    "inverseindex_dict = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dragon\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>anime_description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9728</th>\n",
       "      <td>Chaos Dragon: Sekiryuu Seneki</td>\n",
       "      <td>In 3015, the year of Huanli, two countries, Do...</td>\n",
       "      <td>https://myanimelist.net/anime/30091/Chaos_Drag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8194</th>\n",
       "      <td>Bada-ui Jeonseol Jangbogo</td>\n",
       "      <td>In a not too distant future, the government se...</td>\n",
       "      <td>https://myanimelist.net/anime/10194/Bada-ui_Je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>X/1999</td>\n",
       "      <td>At the millennial edge, the concluding battle ...</td>\n",
       "      <td>https://myanimelist.net/anime/155/X_1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16387</th>\n",
       "      <td>Kiki to Lala no Mamatte Suteki!</td>\n",
       "      <td>Lala finds a myserious egg that hatches into a...</td>\n",
       "      <td>https://myanimelist.net/anime/29249/Kiki_to_La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>Hayate no Gotoku!!</td>\n",
       "      <td>Dr. Saotome returns from the dead after being ...</td>\n",
       "      <td>https://myanimelist.net/anime/4192/Hayate_no_G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>Fairy Tail Movie 2: Dragon Cry</td>\n",
       "      <td>Dragon Cry is a magical artifact of deadly pow...</td>\n",
       "      <td>https://myanimelist.net/anime/30778/Fairy_Tail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>Dragon Ball Kai: Mirai ni Heiwa wo! Goku no Ta...</td>\n",
       "      <td>Special episode about Trunks going back to the...</td>\n",
       "      <td>https://myanimelist.net/anime/11359/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7674</th>\n",
       "      <td>Choegang Top Plate</td>\n",
       "      <td>The storyline is unknown but judging from the ...</td>\n",
       "      <td>https://myanimelist.net/anime/31211/Choegang_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17401</th>\n",
       "      <td>Harem Time The Animation</td>\n",
       "      <td>A special governmental program has been introd...</td>\n",
       "      <td>https://myanimelist.net/anime/14991/Harem_Time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>Rozen Maiden: Meitantei Kunkun - Duell Walzer</td>\n",
       "      <td>Learning to ride and tame dragons comes easy t...</td>\n",
       "      <td>https://myanimelist.net/anime/2237/Rozen_Maide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "9728                       Chaos Dragon: Sekiryuu Seneki   \n",
       "8194                           Bada-ui Jeonseol Jangbogo   \n",
       "5635                                              X/1999   \n",
       "16387                    Kiki to Lala no Mamatte Suteki!   \n",
       "1030                                  Hayate no Gotoku!!   \n",
       "...                                                  ...   \n",
       "1528                      Fairy Tail Movie 2: Dragon Cry   \n",
       "1526   Dragon Ball Kai: Mirai ni Heiwa wo! Goku no Ta...   \n",
       "7674                                  Choegang Top Plate   \n",
       "17401                           Harem Time The Animation   \n",
       "6143       Rozen Maiden: Meitantei Kunkun - Duell Walzer   \n",
       "\n",
       "                                       anime_description  \\\n",
       "9728   In 3015, the year of Huanli, two countries, Do...   \n",
       "8194   In a not too distant future, the government se...   \n",
       "5635   At the millennial edge, the concluding battle ...   \n",
       "16387  Lala finds a myserious egg that hatches into a...   \n",
       "1030   Dr. Saotome returns from the dead after being ...   \n",
       "...                                                  ...   \n",
       "1528   Dragon Cry is a magical artifact of deadly pow...   \n",
       "1526   Special episode about Trunks going back to the...   \n",
       "7674   The storyline is unknown but judging from the ...   \n",
       "17401  A special governmental program has been introd...   \n",
       "6143   Learning to ride and tame dragons comes easy t...   \n",
       "\n",
       "                                                     url  \n",
       "9728   https://myanimelist.net/anime/30091/Chaos_Drag...  \n",
       "8194   https://myanimelist.net/anime/10194/Bada-ui_Je...  \n",
       "5635            https://myanimelist.net/anime/155/X_1999  \n",
       "16387  https://myanimelist.net/anime/29249/Kiki_to_La...  \n",
       "1030   https://myanimelist.net/anime/4192/Hayate_no_G...  \n",
       "...                                                  ...  \n",
       "1528   https://myanimelist.net/anime/30778/Fairy_Tail...  \n",
       "1526   https://myanimelist.net/anime/11359/Dragon_Bal...  \n",
       "7674   https://myanimelist.net/anime/31211/Choegang_T...  \n",
       "17401  https://myanimelist.net/anime/14991/Harem_Time...  \n",
       "6143   https://myanimelist.net/anime/2237/Rozen_Maide...  \n",
       "\n",
       "[195 rows x 3 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input()\n",
    "query = preprocess(query)\n",
    "s = set(map(int, np.linspace(0, 19000, 19001)))\n",
    "for q in query:\n",
    "    s = s.intersection(inverseindex_dict[q])\n",
    "ds.iloc[list(s), [-1, 9, -2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term, desc_preprocessed):\n",
    "    return desc_preprocessed.count(term)/ len(desc_preprocessed)\n",
    "def idf(term, inverseindex_dict):\n",
    "    return np.log(len(inverseindex_dict)/len(inverseindex_dict[term]))\n",
    "def tf_idf(term, desc_preprocessed, inverseindex_dict):\n",
    "    x = tf(term, desc_preprocessed) * idf(term, inverseindex_dict)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_invertedindex_dict = {}\n",
    "for idx, desc in zip(ds.index, ds.anime_description):\n",
    "    if idx % 1000 == 0: \n",
    "        print(\"---->\", idx)\n",
    "    preprocessed_list = preprocess(desc)\n",
    "    for term in preprocessed_list:\n",
    "        if term in tfidf_invertedindex_dict:\n",
    "            tfidf = tf_idf(term, preprocessed_list, inverseindex_dict)\n",
    "            tfidf_invertedindex_dict[term].append((idx, tfidf))\n",
    "        else:\n",
    "            tfidf_invertedindex_dict[term] = []\n",
    "a_file = open(\"tfidf_invertindex.pkl\", \"wb\")\n",
    "pickle.dump(tfidf_invertedindex_dict, a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"tfidf_invertindex.pkl\", \"rb\")\n",
    "tfidf_invertedindex_dict = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_dict = {}\n",
    "for idx, desc in zip(ds.index, ds.anime_description):\n",
    "    if idx % 1000 == 0: print(idx)\n",
    "    desc = preprocess(desc)\n",
    "    desc_vec = {}\n",
    "    for d in desc:\n",
    "        tfidf = 0\n",
    "        for i in tfidf_invertedindex_dict[d]:\n",
    "            if idx == i[0]:\n",
    "                tfidf = i[1]\n",
    "                break\n",
    "        desc_vec[d] = tfidf \n",
    "    desc_dict[idx] = desc_vec\n",
    "desc_dict_file = open(\"desc_dict.pkl\", \"wb\")\n",
    "pickle.dump(desc_dict, desc_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_dict_file = open(\"desc_dict.pkl\", \"rb\")\n",
    "desc_dict = pickle.load(desc_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_dict(dict1, dict2):\n",
    "    s = set(dict1.keys()).intersection(set(dict2.keys()))\n",
    "    c = 0\n",
    "    norm1 = 0\n",
    "    norm2 = 0\n",
    "    for i in s:\n",
    "        c += dict1[i] * dict2[i]\n",
    "    for i in dict1.values():\n",
    "        norm1 += np.power(i, 2)\n",
    "    norm1 = np.sqrt(norm1)\n",
    "    \n",
    "    for i in dict2.values():\n",
    "        norm2 += np.power(i, 2)\n",
    "    norm2 = np.sqrt(norm2)\n",
    "    \n",
    "    return c / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saiyan race\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-213-c07044aa0f89>:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return c / (norm1 * norm2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>anime_description</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>0.443457</td>\n",
       "      <td>Following the defeat of a great adversary, Gok...</td>\n",
       "      <td>https://myanimelist.net/anime/14837/Dragon_Bal...</td>\n",
       "      <td>Dragon Ball Z Movie 14: Kami to Kami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>0.339036</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>0.263309</td>\n",
       "      <td>There are many street races in the Initial D S...</td>\n",
       "      <td>https://myanimelist.net/anime/452/InuYasha_Mov...</td>\n",
       "      <td>InuYasha Movie 1: Toki wo Koeru Omoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>0.255030</td>\n",
       "      <td>As Goku investigates the destruction of the So...</td>\n",
       "      <td>https://myanimelist.net/anime/901/Dragon_Ball_...</td>\n",
       "      <td>Dragon Ball Z Movie 08: Moetsukiro!! Nessen, R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      similarity                                  anime_description  \\\n",
       "1972    0.443457  Following the defeat of a great adversary, Gok...   \n",
       "1437    0.339036  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "1397    0.263309  There are many street races in the Initial D S...   \n",
       "2248    0.255030  As Goku investigates the destruction of the So...   \n",
       "\n",
       "                                                    url  \\\n",
       "1972  https://myanimelist.net/anime/14837/Dragon_Bal...   \n",
       "1437  https://myanimelist.net/anime/986/Dragon_Ball_...   \n",
       "1397  https://myanimelist.net/anime/452/InuYasha_Mov...   \n",
       "2248  https://myanimelist.net/anime/901/Dragon_Ball_...   \n",
       "\n",
       "                                                  title  \n",
       "1972               Dragon Ball Z Movie 14: Kami to Kami  \n",
       "1437  Dragon Ball Z Special 1: Tatta Hitori no Saish...  \n",
       "1397               InuYasha Movie 1: Toki wo Koeru Omoi  \n",
       "2248  Dragon Ball Z Movie 08: Moetsukiro!! Nessen, R...  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input()\n",
    "query = preprocess(query)\n",
    "query_vec = {}\n",
    "for q in query:\n",
    "    query_vec[q] = tf_idf(q, query, inverseindex_dict)\n",
    "sim_dict = {}\n",
    "for idx, desc_tfidf in desc_dict.items():\n",
    "    sim_dict[idx] = cosine_similarity_dict(query_vec, desc_tfidf)\n",
    "sim_ds = pd.DataFrame({'similarity': sim_dict.values()})\n",
    "ds['similarity'] = sim_dict.values()\n",
    "k = 4\n",
    "clean_dict = {k: sim_dict[k] for k in sim_dict.keys() if sim_dict[k] != 0.0 and not np.isnan(sim_dict[k])}\n",
    "clean_dict = {k: v for k, v in sorted(clean_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "l = [i for i in clean_dict.keys()]\n",
    "l = l[-k:]\n",
    "\n",
    "heap = hq.heapify(l)\n",
    "ds.iloc[l, [-1, 9, 14, 15]].sort_values('similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A WAY BETTER SOLUTION (way faster and simpler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dragon\n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "query = preprocess(query)\n",
    "s = set(map(int, np.linspace(0, 19000, 19001)))\n",
    "for q in query:\n",
    "    l = [x[0] for x in tfidf_invertedindex_dict[q]]\n",
    "    s = s.intersection(l)\n",
    "# with open('anime.tsv', 'rb') as anime:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_title</th>\n",
       "      <th>anime_type</th>\n",
       "      <th>anime_num_episodes</th>\n",
       "      <th>anime_aired</th>\n",
       "      <th>anime_status</th>\n",
       "      <th>anime_score</th>\n",
       "      <th>anime_users</th>\n",
       "      <th>anime_rank</th>\n",
       "      <th>anime_popularity</th>\n",
       "      <th>anime_description</th>\n",
       "      <th>anime_related</th>\n",
       "      <th>anime_characters</th>\n",
       "      <th>anime_voices</th>\n",
       "      <th>anime_staff</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>Souten Kouro (Beyond the Heavens) - MyAnimeLis...</td>\n",
       "      <td>TV</td>\n",
       "      <td>26</td>\n",
       "      <td>['Apr 8, 2009', 'Sep 30, 2009']</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>7.27</td>\n",
       "      <td>16,783</td>\n",
       "      <td>#2546</td>\n",
       "      <td>#4326</td>\n",
       "      <td>Souten Kouro's story is based loosely on the e...</td>\n",
       "      <td>['Souten Kouro']</td>\n",
       "      <td>[['Cao', 'Cao'], ['Sun', 'Quan'], ['Liu', 'Bei...</td>\n",
       "      <td>[['Miyano', 'Mamoru'], ['Seki', 'Tomokazu'], [...</td>\n",
       "      <td>[[['Tominaga', 'Tsuneo'], 'Director'], [['Hond...</td>\n",
       "      <td>https://myanimelist.net/anime/5226/Souten_Kouro</td>\n",
       "      <td>Souten Kouro</td>\n",
       "      <td>0.043387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>Miracle Giants Doumu-kun - MyAnimeList.net</td>\n",
       "      <td>TV</td>\n",
       "      <td>49</td>\n",
       "      <td>['Apr 2, 1989', 'Mar 25, 1990']</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1,416</td>\n",
       "      <td>#2720</td>\n",
       "      <td>#10289</td>\n",
       "      <td>Dome Shinjo, a 10-year-old boy who is son of a...</td>\n",
       "      <td>['Miracle Giants Doumu-kun Special']</td>\n",
       "      <td>[['Shinjou', 'Akane'], ['Shinjou', 'Mayumi']]</td>\n",
       "      <td>[['Yukie', 'Rena'], ['Inoue', 'Kikuko']]</td>\n",
       "      <td>[[['Kimura', 'Kyotaro'], 'Producer'], [['Watan...</td>\n",
       "      <td>https://myanimelist.net/anime/5273/Miracle_Gia...</td>\n",
       "      <td>Miracle Giants Doumu-kun</td>\n",
       "      <td>0.056903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>Slime Taoshite 300-nen, Shiranai Uchi ni Level...</td>\n",
       "      <td>TV</td>\n",
       "      <td>12</td>\n",
       "      <td>['Apr 10, 2021', 'Jun 26, 2021']</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>7.02</td>\n",
       "      <td>247,041</td>\n",
       "      <td>#3738</td>\n",
       "      <td>#732</td>\n",
       "      <td>In role-playing games, slimes are usually the ...</td>\n",
       "      <td>['Slime Taoshite 300-nen, Shiranai Uchi ni Lev...</td>\n",
       "      <td>[['Azusa'], ['Laika'], ['Falfa'], ['Halkara'],...</td>\n",
       "      <td>[['Yuuki', 'Aoi'], ['Hondo', 'Kaede'], ['Senbo...</td>\n",
       "      <td>[[['Kimura', 'Nobukage'], 'Director'], [['Moto...</td>\n",
       "      <td>https://myanimelist.net/anime/40586/Slime_Taos...</td>\n",
       "      <td>Slime Taoshite 300-nen, Shiranai Uchi ni Level...</td>\n",
       "      <td>0.068078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Tensei shitara Slime Datta Ken - MyAnimeList.net</td>\n",
       "      <td>TV</td>\n",
       "      <td>24</td>\n",
       "      <td>['Oct 2, 2018', 'Mar 19, 2019']</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>8.11</td>\n",
       "      <td>1,030,225</td>\n",
       "      <td>#416</td>\n",
       "      <td>#100</td>\n",
       "      <td>Thirty-seven-year-old Satoru Mikami is a typic...</td>\n",
       "      <td>['Tensei shitara Slime Datta Ken: Kanwa - Veld...</td>\n",
       "      <td>[['Tempest', 'Rimuru'], ['Nava', 'Milim'], ['D...</td>\n",
       "      <td>[['Okasaki', 'Miho'], ['Hidaka', 'Rina'], ['Sa...</td>\n",
       "      <td>[[['Kikuchi', 'Yasuhito'], 'Director, Animatio...</td>\n",
       "      <td>https://myanimelist.net/anime/37430/Tensei_shi...</td>\n",
       "      <td>Tensei shitara Slime Datta Ken</td>\n",
       "      <td>0.068884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            anime_title anime_type  \\\n",
       "2492  Souten Kouro (Beyond the Heavens) - MyAnimeLis...         TV   \n",
       "2662         Miracle Giants Doumu-kun - MyAnimeList.net         TV   \n",
       "3660  Slime Taoshite 300-nen, Shiranai Uchi ni Level...         TV   \n",
       "406    Tensei shitara Slime Datta Ken - MyAnimeList.net         TV   \n",
       "\n",
       "     anime_num_episodes                       anime_aired     anime_status  \\\n",
       "2492                 26   ['Apr 8, 2009', 'Sep 30, 2009']  Finished Airing   \n",
       "2662                 49   ['Apr 2, 1989', 'Mar 25, 1990']  Finished Airing   \n",
       "3660                 12  ['Apr 10, 2021', 'Jun 26, 2021']  Finished Airing   \n",
       "406                  24   ['Oct 2, 2018', 'Mar 19, 2019']  Finished Airing   \n",
       "\n",
       "      anime_score anime_users anime_rank anime_popularity  \\\n",
       "2492         7.27      16,783      #2546            #4326   \n",
       "2662         7.23       1,416      #2720           #10289   \n",
       "3660         7.02     247,041      #3738             #732   \n",
       "406          8.11   1,030,225       #416             #100   \n",
       "\n",
       "                                      anime_description  \\\n",
       "2492  Souten Kouro's story is based loosely on the e...   \n",
       "2662  Dome Shinjo, a 10-year-old boy who is son of a...   \n",
       "3660  In role-playing games, slimes are usually the ...   \n",
       "406   Thirty-seven-year-old Satoru Mikami is a typic...   \n",
       "\n",
       "                                          anime_related  \\\n",
       "2492                                   ['Souten Kouro']   \n",
       "2662               ['Miracle Giants Doumu-kun Special']   \n",
       "3660  ['Slime Taoshite 300-nen, Shiranai Uchi ni Lev...   \n",
       "406   ['Tensei shitara Slime Datta Ken: Kanwa - Veld...   \n",
       "\n",
       "                                       anime_characters  \\\n",
       "2492  [['Cao', 'Cao'], ['Sun', 'Quan'], ['Liu', 'Bei...   \n",
       "2662      [['Shinjou', 'Akane'], ['Shinjou', 'Mayumi']]   \n",
       "3660  [['Azusa'], ['Laika'], ['Falfa'], ['Halkara'],...   \n",
       "406   [['Tempest', 'Rimuru'], ['Nava', 'Milim'], ['D...   \n",
       "\n",
       "                                           anime_voices  \\\n",
       "2492  [['Miyano', 'Mamoru'], ['Seki', 'Tomokazu'], [...   \n",
       "2662           [['Yukie', 'Rena'], ['Inoue', 'Kikuko']]   \n",
       "3660  [['Yuuki', 'Aoi'], ['Hondo', 'Kaede'], ['Senbo...   \n",
       "406   [['Okasaki', 'Miho'], ['Hidaka', 'Rina'], ['Sa...   \n",
       "\n",
       "                                            anime_staff  \\\n",
       "2492  [[['Tominaga', 'Tsuneo'], 'Director'], [['Hond...   \n",
       "2662  [[['Kimura', 'Kyotaro'], 'Producer'], [['Watan...   \n",
       "3660  [[['Kimura', 'Nobukage'], 'Director'], [['Moto...   \n",
       "406   [[['Kikuchi', 'Yasuhito'], 'Director, Animatio...   \n",
       "\n",
       "                                                    url  \\\n",
       "2492    https://myanimelist.net/anime/5226/Souten_Kouro   \n",
       "2662  https://myanimelist.net/anime/5273/Miracle_Gia...   \n",
       "3660  https://myanimelist.net/anime/40586/Slime_Taos...   \n",
       "406   https://myanimelist.net/anime/37430/Tensei_shi...   \n",
       "\n",
       "                                                  title  similarity  \n",
       "2492                                       Souten Kouro    0.043387  \n",
       "2662                           Miracle Giants Doumu-kun    0.056903  \n",
       "3660  Slime Taoshite 300-nen, Shiranai Uchi ni Level...    0.068078  \n",
       "406                      Tensei shitara Slime Datta Ken    0.068884  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = [x for x in tfidf_invertedindex_dict[q] if x[0] in s]\n",
    "q = [x[0] for x in sorted(q, key=lambda x: x[1])]\n",
    "k = int(input())\n",
    "ds.iloc[q[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define new score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to define a new score that builds and improves upon the previously used cosine similarity: we assign different weights to a series of parameters, namely the cosine similarity, the anime's type (TV, Movie, Music, etc.), the anime's score, the anime's characters and the staff that worked on it. \n",
    "The formula we used looks like this:\n",
    "\n",
    "Custom score =  cos.similarity * 0.4 + type * 0.3 + characters * 0.05 + staff * 0.05\n",
    "\n",
    "\n",
    "Type, characters and staff variables are simply calculated as 1 if any word in the query input matches a word in the respective column, and 0 if no words from the query input are found. We felt that this would be a good way to let users filter for what they are interested in in their search: if I want to find a movie from the Dragon Ball serie, I just have to type \"Dragon Ball Movie\" and the animes classified as movies will be prioritized.\n",
    "For the \"score\" variable we simply consider the absolute value of the anime's score (divided by 10 for normalizing purposes), since we wanted to give priority to animes with a higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx_desc = {}\n",
    "inverted_idx_type = {}\n",
    "inverted_idx_chars = {}\n",
    "inverted_idx_staff = {}\n",
    "for idx, t, chars, staff, desc in zip(ds.index, ds.anime_type, ds.anime_characters, ds.anime_staff, ds.anime_description):\n",
    "    if idx % 1000 == 0: \n",
    "        print(\"---->\", idx)\n",
    "    preprocessed_list = preprocess(desc)\n",
    "    for term in preprocessed_list:\n",
    "        if term in inverted_idx_desc:\n",
    "            tfidf = tf_idf(term, preprocessed_list, inverseindex_dict)\n",
    "            inverted_idx_desc[term].append((idx, tfidf))\n",
    "        else:\n",
    "            inverted_idx_desc[term] = []\n",
    "    \n",
    "    if t in inverted_idx_type:\n",
    "            inverted_idx_type[t].append(idx)\n",
    "    else:\n",
    "        inverted_idx_type[t] = []\n",
    "            \n",
    "    for char_name in chars:\n",
    "        if char_name in inverted_idx_chars:\n",
    "            inverted_idx_chars[char_name].append(idx)\n",
    "        else:\n",
    "            inverted_idx_chars[char_name] = []\n",
    "        \n",
    "    for staff_name in staff:\n",
    "        if staff_name in inverted_idx_staff:\n",
    "            inverted_idx_staff[staff_name].append(idx)\n",
    "        else:\n",
    "            inverted_idx_staff[staff_name] = []   \n",
    "    \n",
    "desc_file = open(\"invertindex_desc.pkl\", \"wb\")\n",
    "pickle.dump(inverted_idx_desc, desc_file)\n",
    "\n",
    "type_file = open(\"invertindex_type.pkl\", \"wb\")\n",
    "pickle.dump(inverted_idx_type, type_file)\n",
    "\n",
    "chars_file = open(\"invertindex_chars.pkl\", \"wb\")\n",
    "pickle.dump(inverted_idx_chars, chars_file)\n",
    "\n",
    "staff_file = open(\"invertindex_staff.pkl\", \"wb\")\n",
    "pickle.dump(inverted_idx_staff, staff_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_file = open(\"invertindex_desc.pkl\", \"rb\")\n",
    "inverted_idx_desc = pickle.load(desc_file)\n",
    "\n",
    "type_file = open(\"invertindex_type.pkl\", \"rb\")\n",
    "inverted_idx_type = pickle.load(type_file)\n",
    "\n",
    "chars_file = open(\"invertindex_chars.pkl\", \"rb\")\n",
    "inverted_idx_chars = pickle.load(chars_file)\n",
    "\n",
    "staff_file = open(\"invertindex_staff.pkl\", \"rb\")\n",
    "inverted_idx_staff = pickle.load(staff_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_dict_file = open(\"desc_dict.pkl\", \"rb\")\n",
    "desc_dict = pickle.load(desc_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, t, chars, staff, desc in zip(ds.index, ds.anime_type, ds.anime_characters, ds.anime_staff, ds.anime_description):\n",
    "    if idx % 1000 == 0: print(idx)\n",
    "    desc = preprocess(desc)\n",
    "    desc_vec = {}\n",
    "    for d in desc:\n",
    "        tfidf = 0\n",
    "        for i in tfidf_invertedindex_dict[d]:\n",
    "            if idx == i[0]:\n",
    "                tfidf = i[1]\n",
    "                break\n",
    "        desc_vec[d] = tfidf \n",
    "    desc_dict[idx] = {'desc': desc_vec, 'chars': chars, 'type': t, 'staff': staff}\n",
    "big_dict_file = open(\"big_dict.pkl\", \"wb\")\n",
    "pickle.dump(desc_dict, big_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dict_file = open(\"big_dict.pkl\", \"rb\")\n",
    "big_dict = pickle.load(big_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saiyan race movie goku\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-213-c07044aa0f89>:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return c / (norm1 * norm2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>anime_description</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>0.099594</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>0.093649</td>\n",
       "      <td>One peaceful afternoon, the Son family and fri...</td>\n",
       "      <td>https://myanimelist.net/anime/22695/Dragon_Bal...</td>\n",
       "      <td>Dragon Ball Z: Summer Vacation Special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>0.090904</td>\n",
       "      <td>Specials included with the original series.\\n\\...</td>\n",
       "      <td>https://myanimelist.net/anime/2520/Dragon_Ball...</td>\n",
       "      <td>Dragon Ball Specials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158</th>\n",
       "      <td>0.089819</td>\n",
       "      <td>A retelling of Dragon Ball's origin with a dif...</td>\n",
       "      <td>https://myanimelist.net/anime/2748/Dash_Kappei</td>\n",
       "      <td>Dash! Kappei</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      similarity                                  anime_description  \\\n",
       "1437    0.099594  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "5547    0.093649  One peaceful afternoon, the Son family and fri...   \n",
       "4890    0.090904  Specials included with the original series.\\n\\...   \n",
       "3158    0.089819  A retelling of Dragon Ball's origin with a dif...   \n",
       "\n",
       "                                                    url  \\\n",
       "1437  https://myanimelist.net/anime/986/Dragon_Ball_...   \n",
       "5547  https://myanimelist.net/anime/22695/Dragon_Bal...   \n",
       "4890  https://myanimelist.net/anime/2520/Dragon_Ball...   \n",
       "3158     https://myanimelist.net/anime/2748/Dash_Kappei   \n",
       "\n",
       "                                                  title  \n",
       "1437  Dragon Ball Z Special 1: Tatta Hitori no Saish...  \n",
       "5547             Dragon Ball Z: Summer Vacation Special  \n",
       "4890                               Dragon Ball Specials  \n",
       "3158                                       Dash! Kappei  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input()\n",
    "query = preprocess(query)\n",
    "query_vec = {}\n",
    "for q in query:\n",
    "    query_vec[q] = tf_idf(q, query, inverseindex_dict)\n",
    "sim_dict = {}\n",
    "for idx, d in big_dict.items():\n",
    "    t, chars, staff, desc = [d['type'], d['chars'], d['staff'], d['desc']]\n",
    "    t = str(t)\n",
    "    t = t.lower()\n",
    "\n",
    "    t_num = 0\n",
    "    if t in q:\n",
    "        t_num = 1\n",
    "    c_num = 0\n",
    "    for c in chars:\n",
    "        for q in query:\n",
    "            if c == q:\n",
    "                c_num += 1\n",
    "    s_num = 0\n",
    "    for s in chars:\n",
    "        for q in query:\n",
    "            if s == q:\n",
    "                s_num += 1\n",
    "    \n",
    "    sim_dict[idx] = cosine_similarity_dict(query_vec, desc) * 0.25 +  t_num * 0.25 + c_num * 0.25 + s_num * 0.25\n",
    "\n",
    "sim_ds = pd.DataFrame({'similarity': sim_dict.values()})\n",
    "ds['similarity'] = sim_dict.values()\n",
    "k = 4\n",
    "clean_dict = {k: sim_dict[k] for k in sim_dict.keys() if sim_dict[k] != 0.0 and not np.isnan(sim_dict[k])}\n",
    "clean_dict = {k: v for k, v in sorted(clean_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "l = [i for i in clean_dict.keys()]\n",
    "l = l[-k:]\n",
    "\n",
    "heap = hq.heapify(l)\n",
    "ds.iloc[l, [-1, 9, 14, 15]].sort_values('similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BONUS: Understanding the anime's reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_parser(article_path, soup):\n",
    "        anime_title = soup.title.text.strip()\n",
    "        try:\n",
    "            anime_review = [x.strip() for x in re.split('(<br>|\\n+)', soup.find('a', text=\"More reviews\").find_next('div').text) if len(x) > 100]\n",
    "        except:\n",
    "            pass\n",
    "        l = [anime_title, anime_review[:5]]\n",
    "        return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ---> 100 ---> pages/page2/article_100.html\n",
      "4 ---> 200 ---> pages/page4/article_200.html\n",
      "6 ---> 300 ---> pages/page6/article_300.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-296-6430f306fa0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"--->\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"--->\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marticle_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                             \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0mrejections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         for (self.markup, self.original_encoding, self.declared_html_encoding,\n\u001b[0m\u001b[0;32m    343\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains_replacement_characters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m              self.builder.prepare_markup(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py\u001b[0m in \u001b[0;36mprepare_markup\u001b[1;34m(self, markup, user_specified_encoding, document_declared_encoding, exclude_encodings)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;31m# Ask UnicodeDammit to sniff the most likely encoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mtry_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0muser_specified_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_declared_encoding\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n\u001b[0m\u001b[0;32m    364\u001b[0m                                exclude_encodings=exclude_encodings)\n\u001b[0;32m    365\u001b[0m         yield (dammit.markup, dammit.original_encoding,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\dammit.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, override_encodings, smart_quotes_to, is_html, exclude_encodings)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m             \u001b[0mmarkup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\dammit.py\u001b[0m in \u001b[0;36mencodings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;31m# encoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchardet_dammit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_usable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtried\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\dammit.py\u001b[0m in \u001b[0;36mchardet_dammit\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mchardet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m#import chardet.constants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m#chardet.constants._debug = 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\chardet\\__init__.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(byte_str)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mbyte_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUniversalDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\chardet\\universaldetector.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLatin1Prober\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprober\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFOUND_IT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m                     self.result = {'encoding': prober.charset_name,\n\u001b[0;32m    213\u001b[0m                                    \u001b[1;34m'confidence'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_confidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\chardet\\charsetgroupprober.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\chardet\\utf8prober.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mcoding_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoding_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcoding_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mMachineState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_ME\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_of_docs = 19125\n",
    "i = 1\n",
    "with open('anime.csv', 'rb') as urls:\n",
    "    urls_ds = pd.read_csv(urls)\n",
    "    with open('anime_review.tsv', 'w', encoding=\"utf-8\", newline='') as anime:\n",
    "        for p in range(1, num_of_pages + 1):\n",
    "            flag = True\n",
    "            if (flag): \n",
    "                article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "                while os.path.isfile(article_path):\n",
    "                    article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "                    if os.path.isfile(article_path):\n",
    "                        if i % 100 == 0:\n",
    "                            print(p, \"--->\", i, \"--->\", article_path)\n",
    "                        with open(article_path, 'rb') as html:\n",
    "                            soup = BeautifulSoup(html, \"html.parser\")\n",
    "                            l = article_parser(article_path, soup)\n",
    "                            l.append(urls_ds.url[i - 1])\n",
    "                            l.append(urls_ds.name[i - 1])\n",
    "                            writer = csv.writer(anime, delimiter='\\t')\n",
    "                            writer.writerow(l)\n",
    "                    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join(review.split(\".\")[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question\n",
    "\n",
    " In this problem, the goal is to maximize the sum of values contained in a list, where each value represents the time that the person who made the appointment wants to spend with the personal trainer, so you want to provide the personal trainer with a program that maximizes the total length of accepted appointments. \n",
    "The important condition is that the personal trainer needs a break between appointments and cannot accept two consecutive bookings. In addition, reservations are considered in the order in which they were made, so that the personal trainer cannot go back and accept past bookings, but must follow the chronological order. \n",
    "\n",
    "Therefore, considering the need for a break between one appointment and another and considering that past appointments cannot be accepted but it is necessary to keep to the chronological order, one solution could be to scroll through the list of bookings twice, that is once starting from the first reservation and then from the index 0, or even, and continuing with step 2 for all even indices and scroll the list a second time starting from the second reservation and then from the index 1, or odd, and proceeding with step 2 for all odd indices. \n",
    "Then add the values of the even and odd indices and compare the obtained values. The path that maximizes the sum, will be preferred. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appointments(appointments):\n",
    "    l1 = []\n",
    "    res1 = 0\n",
    "    l2 = []\n",
    "    res2 = 0\n",
    "    for i in range(len(appointments)):\n",
    "        if ((i % 2) == 0):\n",
    "            res1 += appointments[i]\n",
    "            l1.append(appointments[i])\n",
    "        else:\n",
    "            res2 += appointments[i]\n",
    "            l2.append(appointments[i])\n",
    "    if (res1 > res2):\n",
    "        return l1, res1\n",
    "    else:\n",
    "        return l2, res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list is: [30, 40, 25, 50, 30, 20] \n",
      "\n",
      "The best way to accept reservations in the given order is: [40, 50, 20] with max duration of: 110\n"
     ]
    }
   ],
   "source": [
    "mylist = [30, 40, 25, 50, 30, 20] \n",
    "print('The list is:', mylist, '\\n') \n",
    " \n",
    "print('The best way to accept reservations in the given order is:', appointments(mylist)[0], 'with max duration of:', appointments(mylist)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "homework3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
