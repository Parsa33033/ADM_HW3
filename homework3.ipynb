{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Parsa33033/ADM_HW3/blob/main/homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR-dxW-68OXP",
    "outputId": "f197a5c5-f6a8-4303-a0dd-7fd5215e53cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rkVdjQff8ggD"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5F7tX0Mi81Ir"
   },
   "outputs": [],
   "source": [
    "num_of_pages = 384\n",
    "directory = \"\"\n",
    "links_file = \"anime.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXKNsnC682xk",
    "outputId": "a9369342-4bc3-41aa-e9b8-d40eb636b0d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 384/384 [02:53<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "anime = []\n",
    "\n",
    "                               \n",
    "if not os.path.isfile(links_file):\n",
    "    with open(directory + links_file, \"w\", encoding='utf-8', newline='') as fobj:\n",
    "        fobj.write(\"page,name,url\\n\")\n",
    "        for page in tqdm(range(0, num_of_pages)):\n",
    "            url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "            response = requests.get(url)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for tag in soup.find_all('tr'):\n",
    "                links = tag.find_all('a')\n",
    "                for link in links:        \n",
    "                    if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                        x = (page + 1, link.contents[0], link.get('href'))\n",
    "                        anime.append(x)\n",
    "                        writer = csv.writer(fobj)\n",
    "                        writer.writerow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_dir = directory + \"pages\"\n",
    "page_dir = pages_dir + \"/\" + \"page\"\n",
    "article = \"article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "ZYKNkYqf84p_",
    "outputId": "82b6bd03-2088-4b25-da8c-152586d6766b"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(pages_dir):\n",
    "    os.makedirs(pages_dir)\n",
    "    \n",
    "def urlEncodeNonAscii(b):\n",
    "    return re.sub('[\\x80-\\xFF]', lambda c: '%%%02x' % ord(c.group(0)), b)\n",
    "\n",
    "ds = pd.read_csv(directory + links_file)\n",
    "i = 1\n",
    "for p, u in tqdm(zip(ds.page, ds.url)):\n",
    "    d = page_dir + str(p)\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "    d = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "    \n",
    "    if not os.path.exists(d):\n",
    "#         request = requests.get(u, headers={'Cache-Control': 'no-cache', 'User-Agent': 'Mozilla/5.0'})\n",
    "        req = Request(u, headers={'User-Agent': 'XYZ/3.0'})\n",
    "        webpage = urlopen(req, timeout=100).read()\n",
    "        with open(d, \"w\", encoding='utf-8') as fobj:\n",
    "            fobj.write(webpage)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9WbbqyPD__n"
   },
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(parent):\n",
    "    return ''.join(parent.find_all(text=True, recursive=False)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_parser(article_path, soup):\n",
    "        anime_title = soup.title.text.strip()\n",
    "        anime_type = \"\"\n",
    "        try:\n",
    "            anime_type = getText(soup.find('span', text=\"Type:\").parent.a)\n",
    "        except:\n",
    "            pass\n",
    "        anime_num_episodes = \"\"\n",
    "        try:\n",
    "            anime_num_episodes = getText(soup.find('span', text=\"Episodes:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_aired = \"\"\n",
    "        try:\n",
    "            anime_aired = getText(soup.find('span', text=\"Aired:\").parent).split(\" to \")\n",
    "        except:\n",
    "            pass\n",
    "        anime_status = \"\"\n",
    "        try:\n",
    "            anime_status = getText(soup.find('span', text=\"Status:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_score = \"\"\n",
    "        try:\n",
    "            anime_score = getText(soup.find('span', text=\"Score:\").parent.find_all('span', {'class': 'score-label'})[0])\n",
    "        except:\n",
    "            pass\n",
    "        anime_users = \"\"\n",
    "        try:\n",
    "            anime_users = getText(soup.find('span', text=\"Members:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_rank = \"\"\n",
    "        try:\n",
    "            anime_rank = getText(soup.find('span', text=\"Ranked:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_popularity = \"\"\n",
    "        try:\n",
    "            anime_popularity = getText(soup.find('span', text=\"Popularity:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_description = \"\"\n",
    "        try:\n",
    "            anime_description = getText(soup.find('h2', text=\"Synopsis\").parent.parent.p)\n",
    "        except:\n",
    "            pass\n",
    "        anime_related = []\n",
    "        try: \n",
    "            anime_related = list(set(map(getText,soup.find('h2', text=\"Related Anime\").parent.parent.tr.find_all(lambda tag:tag.name == \"a\" and tag.href != \"\")))) \n",
    "        except: \n",
    "            pass\n",
    "        anime_characters = []\n",
    "        try:\n",
    "            anime_characters = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[1].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "        except:\n",
    "            pass\n",
    "        anime_voices = []\n",
    "        try:\n",
    "            anime_voices = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[0].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "        except:\n",
    "            pass\n",
    "        anime_staff = []\n",
    "        try:\n",
    "            anime_staff = list(map(lambda tr: re.split(\"\\n+\", tr.text.strip()), soup.find('h2', text=\"Staff\").find_next('div').find_all('tr')))\n",
    "        except:\n",
    "            pass\n",
    "        l = [anime_title,\n",
    "                anime_type,\n",
    "                anime_num_episodes,\n",
    "                anime_aired,\n",
    "                anime_status,\n",
    "                anime_score,\n",
    "                anime_users,\n",
    "                anime_rank,\n",
    "                anime_popularity,\n",
    "                anime_description,\n",
    "                anime_related,\n",
    "                anime_characters,\n",
    "                anime_voices,\n",
    "                anime_staff]\n",
    "        return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ---> 100 ---> pages/page2/article_100.html\n"
     ]
    }
   ],
   "source": [
    "num_of_docs = 19125\n",
    "i = 1\n",
    "for p in range(1, num_of_pages + 1):\n",
    "    flag = True\n",
    "    if (flag): \n",
    "        article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "    while os.path.isfile(article_path):\n",
    "        article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "        if os.path.isfile(article_path):\n",
    "            if i % 100 == 0:\n",
    "                print(p, \"--->\", i, \"--->\", article_path)\n",
    "                break\n",
    "            html = open(article_path, 'rb')\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            l = article_parser(article_path, soup)\n",
    "            if not os.path.exists('docs'):\n",
    "                os.makedirs('docs')\n",
    "            anime = open('docs/anime'+str(i)+'.tsv', 'w', encoding=\"utf-8\", newline='')\n",
    "            writer = csv.writer(anime, delimiter='\\t')\n",
    "            writer.writerow(l)\n",
    "        i += 1\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question\n",
    "\n",
    " In this problem, the goal is to maximize the sum of values contained in a list, where each value represents the time that the person who made the appointment wants to spend with the personal trainer, so you want to provide the personal trainer with a program that maximizes the total length of accepted appointments. \n",
    "The important condition is that the personal trainer needs a break between appointments and cannot accept two consecutive bookings. In addition, reservations are considered in the order in which they were made, so that the personal trainer cannot go back and accept past bookings, but must follow the chronological order. \n",
    "\n",
    "Therefore, considering the need for a break between one appointment and another and considering that past appointments cannot be accepted but it is necessary to keep to the chronological order, one solution could be to scroll through the list of bookings twice, that is once starting from the first reservation and then from the index 0, or even, and continuing with step 2 for all even indices and scroll the list a second time starting from the second reservation and then from the index 1, or odd, and proceeding with step 2 for all odd indices. \n",
    "Then add the values of the even and odd indices and compare the obtained values. The path that maximizes the sum, will be preferred. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appointments (appointments):\n",
    "    l1 = []\n",
    "    res1 = 0\n",
    "    l2 = []\n",
    "    res2 = 0\n",
    "    for i in range(len(appointments)):\n",
    "        if ((i % 2) == 0):\n",
    "            res1 += appointments[i]\n",
    "            l1.append(appointments[i])\n",
    "        else:\n",
    "            res2 += appointments[i]\n",
    "            l2.append(appointments[i])\n",
    "    if (res1 > res2):\n",
    "        return l1, res1\n",
    "    else:\n",
    "        return l2, res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list is: [30, 40, 25, 50, 30, 20] \n",
      "\n",
      "The best way to accept reservations in the given order is: [40, 50, 20] with max duration of: 110\n"
     ]
    }
   ],
   "source": [
    "mylist = [30, 40, 25, 50, 30, 20] \n",
    "print('The list is:', mylist, '\\n') \n",
    " \n",
    "print('The best way to accept reservations in the given order is:', appointments(mylist)[0], 'with max duration of:', appointments(mylist)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "homework3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
